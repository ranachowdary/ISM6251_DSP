{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "115dd941",
   "metadata": {},
   "source": [
    "# Shanmuka Rana Prathap Chowdary Ponnaganti U97674115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edf824a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "# set random seed to ensure that results are repeatable\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "548b040d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Income</th>\n",
       "      <th>ZIP Code</th>\n",
       "      <th>Family</th>\n",
       "      <th>CCAvg</th>\n",
       "      <th>Education</th>\n",
       "      <th>Mortgage</th>\n",
       "      <th>Personal Loan</th>\n",
       "      <th>Securities Account</th>\n",
       "      <th>CD Account</th>\n",
       "      <th>Online</th>\n",
       "      <th>CreditCard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>91107</td>\n",
       "      <td>4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>90089</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>94720</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>94112</td>\n",
       "      <td>1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>91330</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>4996</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>92697</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>4997</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>92037</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>4998</td>\n",
       "      <td>63</td>\n",
       "      <td>39</td>\n",
       "      <td>24</td>\n",
       "      <td>93023</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>4999</td>\n",
       "      <td>65</td>\n",
       "      <td>40</td>\n",
       "      <td>49</td>\n",
       "      <td>90034</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>5000</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>83</td>\n",
       "      <td>92612</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  Age  Experience  Income  ZIP Code  Family  CCAvg  Education  \\\n",
       "0        1   25           1      49     91107       4    1.6          1   \n",
       "1        2   45          19      34     90089       3    1.5          1   \n",
       "2        3   39          15      11     94720       1    1.0          1   \n",
       "3        4   35           9     100     94112       1    2.7          2   \n",
       "4        5   35           8      45     91330       4    1.0          2   \n",
       "...    ...  ...         ...     ...       ...     ...    ...        ...   \n",
       "4995  4996   29           3      40     92697       1    1.9          3   \n",
       "4996  4997   30           4      15     92037       4    0.4          1   \n",
       "4997  4998   63          39      24     93023       2    0.3          3   \n",
       "4998  4999   65          40      49     90034       3    0.5          2   \n",
       "4999  5000   28           4      83     92612       3    0.8          1   \n",
       "\n",
       "      Mortgage  Personal Loan  Securities Account  CD Account  Online  \\\n",
       "0            0              0                   1           0       0   \n",
       "1            0              0                   1           0       0   \n",
       "2            0              0                   0           0       0   \n",
       "3            0              0                   0           0       0   \n",
       "4            0              0                   0           0       0   \n",
       "...        ...            ...                 ...         ...     ...   \n",
       "4995         0              0                   0           0       1   \n",
       "4996        85              0                   0           0       1   \n",
       "4997         0              0                   0           0       0   \n",
       "4998         0              0                   0           0       1   \n",
       "4999         0              0                   0           0       1   \n",
       "\n",
       "      CreditCard  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              1  \n",
       "...          ...  \n",
       "4995           0  \n",
       "4996           0  \n",
       "4997           0  \n",
       "4998           0  \n",
       "4999           1  \n",
       "\n",
       "[5000 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('UniversalBank.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dff04c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Age', 'Experience', 'Income', 'ZIP Code', 'Family', 'CCAvg',\n",
       "       'Education', 'Mortgage', 'Personal Loan', 'Securities Account',\n",
       "       'CD Account', 'Online', 'CreditCard'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b45269",
   "metadata": {},
   "source": [
    "# I decided that a couple of variables aren't predictors; therefore we drop them and then check if there are any missing values in the remaining variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be68de4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                   0\n",
       "Experience            0\n",
       "Income                0\n",
       "Family                0\n",
       "CCAvg                 0\n",
       "Education             0\n",
       "Mortgage              0\n",
       "Personal Loan         0\n",
       "Securities Account    0\n",
       "CD Account            0\n",
       "Online                0\n",
       "CreditCard            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop ID, and Zip Code as predictors\n",
    "df = df.drop(columns=['ID', 'ZIP Code'])\n",
    "\n",
    "# check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11d29623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Experience', 'Income', 'Family', 'CCAvg', 'Education',\n",
       "       'Mortgage', 'Personal Loan', 'Securities Account', 'CD Account',\n",
       "       'Online', 'CreditCard'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7226b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Age', 'Experience', 'Income', 'Family', 'CCAvg', 'Education',\n",
    "       'Mortgage', 'Personal Loan', 'Securities Account',\n",
    "       'Online', 'CreditCard']]\n",
    "y = df['CD Account']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb982698",
   "metadata": {},
   "source": [
    "# Modelling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f9cb57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame({\"model\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ce2b2d",
   "metadata": {},
   "source": [
    "# Fitting a SVM classification model using linear kernal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e910408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_lin_model = SVC(kernel=\"linear\")\n",
    "_ = svm_lin_model.fit(X, np.ravel(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2859651",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preds = svm_lin_model.predict(X)\n",
    "c_matrix = confusion_matrix(y, model_preds)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"linear svm\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4027884e",
   "metadata": {},
   "source": [
    "# Fitting a SVM classification model using rbf kernal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9f0eb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_rbf_model = SVC(kernel=\"rbf\", C=10, gamma='scale',probability=True)\n",
    "_ = svm_rbf_model.fit(X, np.ravel(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "720739ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\AppData\\Local\\Temp\\ipykernel_16048\\161462505.py:9: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  'Precision': [TP/(TP+FP)],\n"
     ]
    }
   ],
   "source": [
    "model_preds = svm_rbf_model.predict(X)\n",
    "c_matrix = confusion_matrix(y, model_preds)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"rbf svm\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d276af3",
   "metadata": {},
   "source": [
    "# Fitting a SVM classification model using polynomial kernal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e72bc32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_poly_model = SVC(kernel=\"poly\", degree=3, coef0=1, C=10)\n",
    "_ = svm_poly_model.fit(X, np.ravel(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccb9a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preds = svm_poly_model.predict(X)\n",
    "c_matrix = confusion_matrix(y, model_preds)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"poly svm\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3a50028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linear svm</td>\n",
       "      <td>0.9788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.649007</td>\n",
       "      <td>0.787149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rbf svm</td>\n",
       "      <td>0.9396</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>poly svm</td>\n",
       "      <td>0.9458</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.102649</td>\n",
       "      <td>0.186186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model  Accuracy  Precision    Recall        F1\n",
       "0  linear svm    0.9788        1.0  0.649007  0.787149\n",
       "0     rbf svm    0.9396        NaN  0.000000  0.000000\n",
       "0    poly svm    0.9458        1.0  0.102649  0.186186"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66185ac8",
   "metadata": {},
   "source": [
    "# Decision Trees using RandomSearch and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d719824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(30,36),  \n",
    "    'min_samples_leaf': np.arange(6,12),\n",
    "    'min_impurity_decrease': np.arange(0.0048, 0.0054, 0.0001),\n",
    "    'max_leaf_nodes': np.arange(162,168), \n",
    "    'max_depth': np.arange(15,21), \n",
    "    'criterion': ['entropy'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1309984b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9072 candidates, totalling 45360 fits\n",
      "The best recall score is 0.6359562841530055\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 15, 'max_leaf_nodes': 162, 'min_impurity_decrease': 0.0048, 'min_samples_leaf': 6, 'min_samples_split': 35}\n"
     ]
    }
   ],
   "source": [
    "dtree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X, y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b96795c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9762000 Precision=0.9552239 Recall=0.6357616 F1=0.7634195\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "c_matrix = confusion_matrix(y, grid_search.predict(X))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a153ed36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best recall score is 0.6558469945355191\n",
      "... with parameters: {'min_samples_split': 37, 'min_samples_leaf': 5, 'min_impurity_decrease': 0.0011, 'max_leaf_nodes': 185, 'max_depth': 7, 'criterion': 'entropy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "45 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.         0.63595628 0.2679235  0.23502732 0.63595628 0.63928962\n",
      " 0.26147541 0.63595628 0.63595628 0.57262295 0.2679235  0.60262295\n",
      " 0.63595628 0.62928962 0.64912568 0.64912568 0.35459016 0.65245902\n",
      " 0.63595628 0.63262295 0.4404918  0.55928962 0.63595628 0.54928962\n",
      " 0.53617486 0.63595628 0.56928962 0.53295082 0.57262295 0.53961749\n",
      " 0.63595628 0.39382514 0.35459016 0.63595628        nan 0.53295082\n",
      " 0.55928962 0.24169399 0.63595628 0.26803279 0.63595628 0.60262295\n",
      " 0.63595628 0.56595628 0.55928962 0.63595628 0.         0.57262295\n",
      " 0.63595628 0.62928962 0.63595628 0.63595628 0.63595628 0.63595628\n",
      " 0.60262295 0.60262295 0.63595628 0.2679235  0.55928962 0.63595628\n",
      " 0.54639344 0.62928962 0.63595628 0.4404918  0.63595628 0.63595628\n",
      " 0.52306011 0.60262295 0.3604918  0.2679235  0.63595628 0.63595628\n",
      " 0.54928962 0.55928962 0.65579235 0.25163934 0.60262295 0.54928962\n",
      " 0.62262295 0.63595628 0.63595628 0.2647541  0.33415301 0.55601093\n",
      " 0.33415301 0.63595628 0.25163934 0.39382514 0.4404918  0.63595628\n",
      " 0.39382514 0.63595628 0.62928962 0.57590164 0.60262295 0.55928962\n",
      " 0.2679235  0.63595628 0.29781421 0.63595628 0.57262295 0.54928962\n",
      " 0.55928962 0.55928962 0.26147541 0.52306011 0.64912568 0.4404918\n",
      " 0.63595628        nan 0.25163934 0.56595628 0.60262295 0.4404918\n",
      " 0.31721311 0.63595628 0.26125683 0.46295082 0.53961749 0.3479235\n",
      " 0.25163934 0.6557377  0.3479235  0.60262295 0.63595628 0.63595628\n",
      " 0.63595628 0.63262295 0.63595628 0.60262295 0.63595628 0.63595628\n",
      " 0.54928962 0.63595628 0.2679235  0.63595628 0.63595628 0.54639344\n",
      " 0.63595628 0.60262295 0.55262295 0.63595628 0.63595628 0.63595628\n",
      " 0.35459016 0.54956284 0.57262295 0.53617486 0.63595628 0.65579235\n",
      " 0.60262295 0.34081967 0.63595628 0.63595628 0.53295082 0.60262295\n",
      " 0.63595628 0.48289617 0.63595628 0.57262295 0.65245902        nan\n",
      " 0.         0.53617486 0.57262295 0.57590164 0.52306011 0.54956284\n",
      " 0.63595628 0.56928962 0.63595628 0.63595628 0.63595628 0.63595628\n",
      " 0.24814208 0.4404918  0.26147541 0.63595628        nan 0.3479235\n",
      " 0.55928962 0.63595628 0.63595628 0.52306011 0.63595628 0.57590164\n",
      " 0.63595628 0.49989071 0.63595628 0.4404918  0.56595628 0.63595628\n",
      " 0.53595628 0.39382514 0.62262295 0.         0.63595628 0.24169399\n",
      " 0.63595628 0.63595628 0.54928962 0.52306011 0.54639344 0.37382514\n",
      " 0.63595628 0.60262295 0.35382514 0.53295082 0.4404918  0.54928962\n",
      " 0.55928962 0.60262295 0.56595628 0.2679235  0.57262295        nan\n",
      " 0.2647541  0.63595628 0.2679235  0.63595628        nan 0.63595628\n",
      " 0.63595628 0.32125683 0.63595628 0.52961749 0.63595628 0.39382514\n",
      " 0.63595628 0.60262295 0.63595628 0.57590164 0.54928962 0.63595628\n",
      " 0.24836066 0.63595628 0.55928962 0.2679235  0.54639344 0.55928962\n",
      " 0.63595628 0.54289617 0.63595628 0.2647541  0.63595628 0.29781421\n",
      " 0.         0.63595628 0.63595628 0.60262295 0.55928962 0.63928962\n",
      " 0.54928962 0.57590164 0.63595628 0.57262295 0.63595628 0.46295082\n",
      " 0.53295082 0.57590164 0.54928962 0.63595628 0.63595628 0.54928962\n",
      " 0.63595628 0.63595628 0.63595628 0.49956284 0.63595628 0.63595628\n",
      " 0.57262295 0.60262295 0.         0.50595628 0.57590164 0.63595628\n",
      " 0.26147541 0.63595628 0.63595628 0.64912568 0.60262295 0.60262295\n",
      " 0.4404918  0.2679235  0.60262295 0.63595628 0.53295082 0.2647541\n",
      " 0.29781421 0.3479235  0.24169399 0.62928962 0.54956284 0.\n",
      " 0.54928962 0.52306011 0.2679235  0.63595628 0.63595628 0.3204918\n",
      " 0.60262295 0.63595628 0.63595628 0.63595628 0.63595628 0.57262295\n",
      " 0.56928962 0.54928962 0.46677596 0.3479235  0.62928962 0.63595628\n",
      " 0.2647541  0.63595628 0.62928962 0.63595628 0.56273224 0.63595628\n",
      " 0.65579235 0.63595628 0.60262295 0.64912568 0.63590164 0.65245902\n",
      " 0.2679235  0.63251366 0.3479235  0.39382514 0.63595628 0.63595628\n",
      " 0.3479235  0.55928962 0.54928962 0.63595628 0.63595628        nan\n",
      " 0.52961749 0.2679235  0.63595628 0.3479235  0.62928962 0.63595628\n",
      " 0.57262295 0.55928962 0.57262295 0.63595628 0.56595628 0.26147541\n",
      " 0.63595628 0.63595628 0.63595628 0.63595628 0.53295082 0.2647541\n",
      " 0.60262295 0.63595628 0.57590164        nan 0.63595628 0.4404918\n",
      " 0.63595628 0.54928962 0.54928962 0.31459016 0.2679235  0.35459016\n",
      " 0.55262295 0.63595628 0.26147541 0.63595628 0.62928962 0.63595628\n",
      " 0.         0.55928962 0.25163934 0.63595628 0.55928962 0.56595628\n",
      " 0.2679235  0.3479235         nan 0.4404918  0.63595628 0.63595628\n",
      " 0.55928962 0.63595628 0.39382514 0.2679235  0.31459016 0.\n",
      " 0.4404918  0.63595628 0.63595628 0.63595628 0.31054645 0.53961749\n",
      " 0.54928962 0.65579235 0.63595628 0.60262295 0.63595628 0.54928962\n",
      " 0.45371585 0.63595628 0.50655738 0.60262295 0.54928962 0.26147541\n",
      " 0.60262295 0.26147541 0.3479235  0.26147541 0.62262295 0.63595628\n",
      " 0.63595628 0.31054645 0.63595628 0.2647541  0.3479235  0.26147541\n",
      " 0.26147541 0.63595628 0.60262295 0.35459016 0.39382514 0.46677596\n",
      " 0.4404918  0.63595628 0.60262295 0.53295082 0.29781421 0.35459016\n",
      " 0.63595628 0.55601093 0.2679235  0.63595628 0.2679235  0.60262295\n",
      " 0.56595628 0.56595628 0.26803279 0.60262295 0.60262295 0.64912568\n",
      " 0.63595628 0.49989071 0.63595628 0.26803279 0.54928962 0.56595628\n",
      " 0.63595628 0.2679235  0.55928962 0.53295082 0.63595628 0.33415301\n",
      " 0.55928962 0.57262295 0.65584699 0.54928962 0.63595628 0.63595628\n",
      " 0.55928962 0.52644809 0.52644809 0.54928962 0.2647541  0.63595628\n",
      " 0.63595628 0.57590164 0.62928962 0.63595628 0.63595628 0.55601093\n",
      " 0.2679235  0.63595628 0.32125683 0.63595628 0.63595628 0.63595628\n",
      " 0.63595628 0.29781421 0.57590164 0.32125683 0.39382514 0.63595628\n",
      " 0.63256831 0.63595628 0.62595628 0.54289617 0.24169399 0.53295082\n",
      " 0.62262295 0.3479235  0.26147541 0.63595628 0.63595628 0.25163934\n",
      " 0.53295082 0.57262295]\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.         0.63577381 0.34529337 0.28723638 0.64321182 0.64569459\n",
      " 0.32866157 0.63577381 0.63577381 0.56784404 0.34529337 0.6159391\n",
      " 0.63577381 0.62748877 0.658105   0.64901409 0.40810329 0.65066699\n",
      " 0.63577381 0.64156236 0.50253078 0.54800933 0.63577381 0.57296389\n",
      " 0.54806762 0.63577381 0.5927986  0.52731045 0.56784404 0.53640479\n",
      " 0.64736806 0.45129111 0.40810329 0.63577381        nan 0.52731045\n",
      " 0.54800933 0.29550084 0.63577381 0.31704331 0.63577381 0.6159391\n",
      " 0.64321182 0.55627379 0.54800933 0.63577381 0.         0.56784404\n",
      " 0.63577381 0.63908302 0.63577381 0.63577381 0.63577381 0.63577381\n",
      " 0.6159391  0.6159391  0.63577381 0.34529337 0.54800933 0.63577381\n",
      " 0.53883612 0.63908302 0.64321182 0.50253078 0.63577381 0.63577381\n",
      " 0.52317136 0.6159391  0.4314564  0.34529337 0.64158294 0.63577381\n",
      " 0.57296389 0.54800933 0.6498474  0.31206406 0.6159391  0.57296389\n",
      " 0.63081513 0.63577381 0.63577381 0.30874456 0.39241796 0.54468983\n",
      " 0.39241796 0.63577381 0.31206406 0.45129111 0.50253078 0.63577381\n",
      " 0.45129111 0.64321182 0.63908302 0.57614279 0.6159391  0.54800933\n",
      " 0.34529337 0.64321182 0.3269641  0.63577381 0.55875313 0.57296389\n",
      " 0.54800933 0.54800933 0.32866157 0.52317136 0.64901409 0.50253078\n",
      " 0.63577381        nan 0.31206406 0.55048867 0.6159391  0.50253078\n",
      " 0.36268646 0.63577381 0.33702891 0.48268578 0.53557491 0.39983883\n",
      " 0.31206406 0.65730942 0.39983883 0.6159391  0.63577381 0.63577381\n",
      " 0.63577381 0.64156236 0.63577381 0.6159391  0.63577381 0.63577381\n",
      " 0.57296389 0.63577381 0.34529337 0.63577381 0.63577381 0.53883612\n",
      " 0.63577381 0.6159391  0.54635643 0.63577381 0.63577381 0.63577381\n",
      " 0.40810329 0.53635678 0.55875313 0.54806762 0.63577381 0.6498474\n",
      " 0.6159391  0.40068242 0.63577381 0.63577381 0.52731045 0.6159391\n",
      " 0.63577381 0.49922842 0.63577381 0.56784404 0.65066699        nan\n",
      " 0.         0.54806762 0.56784404 0.56705188 0.52317136 0.54550941\n",
      " 0.63577381 0.5927986  0.63577381 0.63577381 0.63577381 0.63577381\n",
      " 0.31213264 0.50253078 0.32866157 0.63577381        nan 0.39983883\n",
      " 0.54800933 0.64158294 0.63577381 0.52317136 0.63577381 0.57614279\n",
      " 0.63577381 0.49502075 0.63577381 0.50253078 0.55627379 0.63577381\n",
      " 0.57296389 0.45129111 0.63081513 0.         0.63577381 0.29550084\n",
      " 0.63577381 0.63577381 0.57296389 0.52317136 0.53966599 0.42470423\n",
      " 0.63577381 0.6159391  0.43972086 0.52814032 0.50253078 0.57296389\n",
      " 0.54800933 0.6159391  0.55627379 0.34529337 0.55875313        nan\n",
      " 0.30874456 0.63577381 0.34529337 0.63577381        nan 0.63577381\n",
      " 0.63577381 0.38826858 0.64321182 0.52566099 0.63577381 0.45129111\n",
      " 0.63577381 0.6159391  0.63577381 0.57614279 0.57296389 0.64238881\n",
      " 0.3037653  0.63577381 0.54800933 0.34529337 0.53966599 0.55131511\n",
      " 0.63577381 0.53141182 0.63577381 0.30874456 0.63577381 0.3269641\n",
      " 0.         0.63577381 0.63577381 0.6159391  0.54800933 0.64569459\n",
      " 0.57296389 0.57614279 0.63577381 0.56784404 0.63577381 0.48268578\n",
      " 0.52814032 0.56705188 0.57296389 0.63577381 0.63577381 0.57296389\n",
      " 0.63577381 0.63577381 0.63577381 0.49505161 0.63577381 0.63577381\n",
      " 0.55875313 0.6159391  0.         0.51247214 0.57614279 0.63577381\n",
      " 0.32866157 0.63577381 0.63577381 0.64901409 0.6159391  0.6159391\n",
      " 0.50253078 0.34529337 0.6159391  0.63577381 0.52814032 0.30874456\n",
      " 0.3269641  0.39983883 0.29550084 0.63492679 0.53801653 0.\n",
      " 0.57296389 0.52317136 0.34529337 0.63577381 0.63577381 0.37098522\n",
      " 0.6159391  0.63577381 0.63577381 0.63577381 0.63577381 0.56784404\n",
      " 0.5927986  0.57296389 0.47679435 0.39983883 0.63908302 0.63577381\n",
      " 0.30874456 0.63577381 0.63492679 0.63577381 0.56207263 0.63577381\n",
      " 0.6498474  0.63577381 0.6159391  0.65397277 0.66720963 0.65066699\n",
      " 0.34529337 0.64074963 0.39983883 0.45129111 0.63577381 0.63577381\n",
      " 0.39983883 0.54800933 0.57296389 0.63577381 0.63577381        nan\n",
      " 0.52566099 0.34529337 0.63577381 0.39983883 0.63908302 0.63577381\n",
      " 0.55875313 0.55131511 0.55875313 0.63577381 0.55627379 0.32866157\n",
      " 0.63577381 0.64321182 0.63577381 0.63577381 0.52731045 0.30874456\n",
      " 0.6159391  0.63577381 0.57614279        nan 0.63577381 0.50253078\n",
      " 0.63577381 0.57296389 0.57296389 0.38000412 0.34529337 0.40810329\n",
      " 0.54635643 0.64321182 0.32866157 0.63577381 0.63908302 0.63577381\n",
      " 0.         0.54800933 0.31206406 0.63577381 0.54800933 0.55627379\n",
      " 0.34529337 0.39983883        nan 0.50253078 0.63577381 0.63577381\n",
      " 0.54800933 0.63577381 0.45129111 0.34529337 0.38000412 0.\n",
      " 0.50253078 0.63577381 0.63577381 0.63577381 0.354422   0.52892219\n",
      " 0.57296389 0.6498474  0.63577381 0.6159391  0.63577381 0.57296389\n",
      " 0.50749974 0.63577381 0.50328521 0.6159391  0.57296389 0.32866157\n",
      " 0.6159391  0.32866157 0.39983883 0.32866157 0.63081513 0.63577381\n",
      " 0.63577381 0.354422   0.64158294 0.30874456 0.39983883 0.32866157\n",
      " 0.32866157 0.63577381 0.6159391  0.40810329 0.45129111 0.47679435\n",
      " 0.50253078 0.63577381 0.6159391  0.52814032 0.3269641  0.40810329\n",
      " 0.63577381 0.54468983 0.34529337 0.63577381 0.34529337 0.6159391\n",
      " 0.55627379 0.55048867 0.32613422 0.6159391  0.6159391  0.64984397\n",
      " 0.63577381 0.49502075 0.63577381 0.32613422 0.57296389 0.55048867\n",
      " 0.63577381 0.34529337 0.54800933 0.52731045 0.63577381 0.39241796\n",
      " 0.54800933 0.56784404 0.65645211 0.57296389 0.63577381 0.64158294\n",
      " 0.54800933 0.53224169 0.53224169 0.57296389 0.30874456 0.63577381\n",
      " 0.63577381 0.57614279 0.63908302 0.63577381 0.63577381 0.54468983\n",
      " 0.34529337 0.63577381 0.38826858 0.63577381 0.63577381 0.63577381\n",
      " 0.63577381 0.3269641  0.57614279 0.38826858 0.45129111 0.64321182\n",
      " 0.65231988 0.63577381 0.63743013 0.53141182 0.29550084 0.52731045\n",
      " 0.63081513 0.39983883 0.32866157 0.63577381 0.63577381 0.31206406\n",
      " 0.52814032 0.56784404]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,55),  \n",
    "    'min_samples_leaf': np.arange(1,55),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 200), \n",
    "    'max_depth': np.arange(1,50), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X, y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36e8c619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9788000 Precision=1.0000000 Recall=0.6490066 F1=0.7871486\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "c_matrix = confusion_matrix(y, rand_search.predict(X))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe81b30",
   "metadata": {},
   "source": [
    "# Logistic Regression using RandomSearch and Grid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e1f6cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best recall score is 0.6491256830601093\n",
      "... with parameters: {'solver': 'liblinear', 'penalty': 'l2', 'max_iter': 830, 'C': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "990 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "310 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1471, in fit\n",
      "    raise ValueError(\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "370 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 464, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "310 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.00661202 0.\n",
      " 0.00661202 0.53666667        nan        nan 0.00661202 0.63256831\n",
      " 0.53666667 0.00661202 0.00333333 0.64912568 0.         0.00333333\n",
      "        nan        nan 0.00661202 0.00661202        nan 0.64912568\n",
      " 0.         0.00661202 0.00661202        nan        nan        nan\n",
      "        nan 0.00661202 0.00661202 0.00333333        nan 0.00661202\n",
      "        nan        nan 0.         0.53666667        nan 0.00661202\n",
      " 0.00333333        nan        nan 0.64912568 0.64912568 0.00661202\n",
      " 0.         0.64912568        nan 0.                nan 0.00661202\n",
      "        nan 0.00661202 0.00333333        nan 0.00661202 0.00661202\n",
      " 0.53666667 0.00661202        nan 0.63256831 0.00661202 0.\n",
      " 0.01983607 0.01983607 0.64912568        nan 0.                nan\n",
      " 0.00333333        nan 0.00333333        nan        nan 0.00333333\n",
      " 0.00661202 0.00661202        nan 0.00333333 0.00661202        nan\n",
      " 0.63256831 0.00333333 0.00661202 0.00661202 0.         0.00661202\n",
      " 0.                nan 0.00661202        nan 0.         0.00661202\n",
      "        nan 0.53666667        nan        nan 0.00333333 0.00661202\n",
      "        nan 0.         0.00661202 0.53666667 0.01983607 0.00661202\n",
      " 0.64912568 0.00661202 0.         0.00661202        nan 0.64912568\n",
      "        nan 0.         0.01983607 0.00661202 0.         0.53666667\n",
      " 0.63256831        nan        nan 0.01983607        nan 0.00333333\n",
      " 0.00661202 0.00661202        nan 0.00661202 0.00661202 0.64912568\n",
      " 0.00661202 0.63256831        nan        nan        nan 0.00661202\n",
      " 0.00661202 0.00661202        nan 0.         0.64912568        nan\n",
      " 0.00661202 0.00661202 0.64912568        nan 0.         0.\n",
      " 0.00661202 0.00661202        nan 0.00661202 0.01983607        nan\n",
      "        nan        nan        nan        nan        nan 0.64912568\n",
      " 0.64912568 0.                nan        nan 0.64912568 0.00661202\n",
      "        nan 0.64912568 0.00333333        nan 0.00661202 0.63256831\n",
      " 0.53666667 0.                nan 0.64912568 0.00661202 0.00333333\n",
      "        nan 0.64912568 0.64912568        nan        nan        nan\n",
      "        nan 0.64912568 0.         0.00661202        nan 0.\n",
      "        nan        nan 0.00661202        nan 0.00333333        nan\n",
      "        nan 0.64912568 0.64912568        nan 0.00333333 0.63256831\n",
      " 0.64912568 0.64912568        nan        nan 0.00661202        nan\n",
      "        nan 0.00661202 0.64912568 0.00661202 0.00333333 0.\n",
      " 0.00661202 0.01983607        nan 0.         0.53666667 0.00661202\n",
      " 0.00661202        nan 0.64912568 0.00661202 0.         0.64912568\n",
      "        nan        nan        nan 0.                nan 0.00333333\n",
      " 0.00333333        nan        nan        nan        nan 0.00661202\n",
      " 0.64912568 0.00661202 0.64912568        nan 0.00661202        nan\n",
      "        nan        nan        nan 0.                nan 0.64912568\n",
      "        nan 0.00661202 0.63256831 0.63256831        nan        nan\n",
      "        nan 0.64912568 0.00661202        nan 0.00661202 0.00661202\n",
      "        nan 0.01983607        nan 0.00333333        nan 0.00661202\n",
      " 0.00661202        nan 0.64912568 0.00661202 0.00661202        nan\n",
      "        nan        nan        nan 0.64912568        nan        nan\n",
      " 0.                nan        nan 0.00661202 0.64912568 0.01983607\n",
      "        nan        nan 0.00661202 0.00333333 0.64912568 0.53666667\n",
      "        nan 0.00661202 0.         0.                nan        nan\n",
      " 0.00661202 0.         0.00661202 0.64912568 0.64912568        nan\n",
      "        nan 0.         0.00661202 0.                nan 0.64912568\n",
      " 0.00333333 0.         0.00661202        nan        nan 0.\n",
      "        nan        nan        nan 0.                nan 0.00661202\n",
      " 0.00661202 0.00661202        nan 0.         0.00661202 0.64912568\n",
      " 0.64912568        nan 0.         0.00333333        nan 0.64912568\n",
      "        nan        nan 0.         0.         0.00661202        nan\n",
      "        nan 0.63256831 0.         0.         0.00661202        nan\n",
      "        nan        nan        nan        nan        nan 0.63256831\n",
      "        nan 0.00661202 0.00661202        nan        nan 0.00333333\n",
      "        nan 0.64912568        nan        nan        nan 0.\n",
      " 0.00661202 0.00661202        nan 0.         0.00661202        nan\n",
      "        nan 0.00661202 0.63256831        nan 0.00661202        nan\n",
      " 0.00333333 0.00333333        nan 0.                nan 0.64912568\n",
      " 0.64912568        nan        nan 0.64912568        nan        nan\n",
      " 0.64912568 0.         0.00333333 0.64912568        nan 0.\n",
      "        nan        nan 0.                nan 0.53666667 0.00333333\n",
      "        nan 0.64912568 0.00661202        nan        nan        nan\n",
      " 0.00333333 0.64912568 0.00661202 0.00661202        nan 0.00661202\n",
      " 0.         0.                nan        nan 0.63256831 0.\n",
      " 0.                nan 0.64912568 0.64912568 0.00661202 0.00661202\n",
      "        nan        nan 0.                nan 0.00661202 0.63256831\n",
      " 0.00661202        nan 0.00661202 0.00661202        nan        nan\n",
      "        nan        nan 0.00661202        nan 0.53666667 0.64912568\n",
      "        nan 0.         0.64912568        nan        nan        nan\n",
      " 0.         0.00661202 0.         0.00661202 0.00333333        nan\n",
      "        nan 0.00333333        nan 0.00661202 0.00333333 0.\n",
      " 0.53666667        nan        nan        nan 0.                nan\n",
      "        nan 0.00661202 0.00661202 0.64912568 0.                nan\n",
      "        nan 0.00661202 0.00661202        nan 0.64912568 0.00661202\n",
      "        nan 0.00333333 0.01983607 0.00333333 0.         0.\n",
      " 0.00333333        nan        nan 0.00333333 0.                nan\n",
      " 0.         0.         0.01983607        nan 0.00661202        nan\n",
      " 0.00661202        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [       nan        nan        nan        nan 0.00827818 0.\n",
      " 0.00662186 0.54885978        nan        nan 0.00579541 0.63577038\n",
      " 0.54885978 0.00496897 0.00414252 0.64901409 0.         0.00414252\n",
      "        nan        nan 0.00662186 0.00662186        nan 0.64901409\n",
      " 0.         0.00662186 0.00827818        nan        nan        nan\n",
      "        nan 0.00827818 0.00662186 0.00414252        nan 0.00662186\n",
      "        nan        nan 0.         0.54885978        nan 0.00662186\n",
      " 0.00331264        nan        nan 0.64901409 0.64901409 0.00827818\n",
      " 0.         0.64901409        nan 0.                nan 0.00496897\n",
      "        nan 0.0074483  0.00414252        nan 0.00662186 0.00827818\n",
      " 0.54885978 0.00662186        nan 0.63577038 0.00662186 0.\n",
      " 0.0215219  0.0215219  0.64901409        nan 0.                nan\n",
      " 0.00414252        nan 0.00331264        nan        nan 0.00414252\n",
      " 0.00414252 0.00662186        nan 0.00414252 0.00496897        nan\n",
      " 0.63577038 0.00414252 0.00662186 0.00662186 0.         0.00496897\n",
      " 0.                nan 0.00827818        nan 0.         0.00662186\n",
      "        nan 0.54885978        nan        nan 0.00414252 0.00414252\n",
      "        nan 0.         0.00662186 0.54885978 0.0215219  0.00662186\n",
      " 0.64901409 0.00662186 0.         0.00662186        nan 0.64901409\n",
      "        nan 0.         0.0215219  0.00827818 0.         0.54885978\n",
      " 0.63577038        nan        nan 0.0215219         nan 0.00414252\n",
      " 0.00496897 0.00662186        nan 0.00662186 0.00662186 0.64901409\n",
      " 0.00496897 0.63577038        nan        nan        nan 0.00496897\n",
      " 0.00662186 0.00827818        nan 0.         0.64901409        nan\n",
      " 0.00662186 0.00662186 0.64901409        nan 0.         0.\n",
      " 0.00662186 0.0074483         nan 0.00827818 0.0215219         nan\n",
      "        nan        nan        nan        nan        nan 0.64901409\n",
      " 0.64901409 0.                nan        nan 0.64901409 0.00827818\n",
      "        nan 0.64901409 0.00414252        nan 0.00662186 0.63577038\n",
      " 0.54885978 0.                nan 0.64901409 0.0074483  0.00414252\n",
      "        nan 0.64901409 0.64901409        nan        nan        nan\n",
      "        nan 0.64901409 0.         0.00662186        nan 0.\n",
      "        nan        nan 0.00662186        nan 0.00414252        nan\n",
      "        nan 0.64901409 0.64901409        nan 0.00414252 0.63577038\n",
      " 0.64901409 0.64901409        nan        nan 0.0074483         nan\n",
      "        nan 0.0074483  0.64901409 0.00662186 0.00331264 0.\n",
      " 0.00496897 0.0215219         nan 0.         0.54885978 0.00496897\n",
      " 0.00827818        nan 0.64901409 0.0074483  0.         0.64901409\n",
      "        nan        nan        nan 0.                nan 0.00414252\n",
      " 0.00414252        nan        nan        nan        nan 0.0074483\n",
      " 0.64901409 0.0074483  0.64901409        nan 0.00662186        nan\n",
      "        nan        nan        nan 0.                nan 0.64901409\n",
      "        nan 0.00496897 0.63577038 0.63577038        nan        nan\n",
      "        nan 0.64901409 0.00414252        nan 0.00827818 0.0074483\n",
      "        nan 0.0215219         nan 0.00331264        nan 0.0074483\n",
      " 0.00827818        nan 0.64901409 0.00662186 0.00414252        nan\n",
      "        nan        nan        nan 0.64901409        nan        nan\n",
      " 0.                nan        nan 0.00662186 0.64901409 0.0215219\n",
      "        nan        nan 0.00827818 0.00331264 0.64901409 0.54885978\n",
      "        nan 0.00662186 0.         0.                nan        nan\n",
      " 0.00496897 0.         0.0074483  0.64901409 0.64901409        nan\n",
      "        nan 0.         0.00496897 0.                nan 0.64901409\n",
      " 0.0024862  0.         0.0074483         nan        nan 0.\n",
      "        nan        nan        nan 0.                nan 0.00662186\n",
      " 0.00662186 0.0074483         nan 0.         0.00827818 0.64901409\n",
      " 0.64901409        nan 0.         0.00414252        nan 0.64901409\n",
      "        nan        nan 0.         0.         0.00662186        nan\n",
      "        nan 0.63577038 0.         0.         0.00662186        nan\n",
      "        nan        nan        nan        nan        nan 0.63577038\n",
      "        nan 0.00827818 0.0074483         nan        nan 0.00414252\n",
      "        nan 0.64901409        nan        nan        nan 0.\n",
      " 0.00496897 0.00827818        nan 0.         0.0074483         nan\n",
      "        nan 0.00662186 0.63577038        nan 0.00496897        nan\n",
      " 0.00414252 0.00414252        nan 0.                nan 0.64901409\n",
      " 0.64901409        nan        nan 0.64901409        nan        nan\n",
      " 0.64901409 0.         0.00331264 0.64901409        nan 0.\n",
      "        nan        nan 0.                nan 0.54885978 0.00414252\n",
      "        nan 0.64901409 0.0074483         nan        nan        nan\n",
      " 0.00414252 0.64901409 0.00579541 0.00662186        nan 0.00414252\n",
      " 0.         0.                nan        nan 0.63577038 0.\n",
      " 0.                nan 0.64901409 0.64901409 0.0074483  0.00496897\n",
      "        nan        nan 0.                nan 0.00827818 0.63577038\n",
      " 0.00496897        nan 0.00662186 0.00662186        nan        nan\n",
      "        nan        nan 0.00662186        nan 0.54885978 0.64901409\n",
      "        nan 0.         0.64901409        nan        nan        nan\n",
      " 0.         0.00827818 0.         0.00579541 0.00414252        nan\n",
      "        nan 0.00414252        nan 0.00662186 0.00331264 0.\n",
      " 0.54885978        nan        nan        nan 0.                nan\n",
      "        nan 0.0074483  0.00662186 0.64901409 0.                nan\n",
      "        nan 0.0074483  0.00496897        nan 0.64901409 0.00662186\n",
      "        nan 0.00331264 0.0215219  0.0024862  0.         0.\n",
      " 0.00414252        nan        nan 0.00414252 0.                nan\n",
      " 0.         0.         0.0215219         nan 0.00662186        nan\n",
      " 0.00662186        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {'C':[0.001,0.01,0.1,1,10], # C is the regulization strength\n",
    "               'penalty':['l1', 'l2','elasticnet','none'],\n",
    "              'solver':['saga','liblinear'],\n",
    "              'max_iter': np.arange(500,1000)\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "lr = LogisticRegression()\n",
    "rand_search = RandomizedSearchCV(estimator = lr, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X,y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestlr = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "125902a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9762000 Precision=0.9552239 Recall=0.6357616 F1=0.7634195\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "c_matrix = confusion_matrix(y, grid_search.predict(X))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c0307d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1600 candidates, totalling 8000 fits\n",
      "The best recall score is 0.6491256830601093\n",
      "... with parameters: {'C': 9, 'max_iter': 430, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "best_penality = rand_search.best_params_['penalty']\n",
    "best_solver = rand_search.best_params_['solver']\n",
    "min_regulization_strength=rand_search.best_params_['C']\n",
    "min_iter = rand_search.best_params_['max_iter']\n",
    "\n",
    "#Using the best parameters from the Random Search to use as range for the parameters to do the grid search\n",
    "param_grid = {\n",
    "    \n",
    "    'C':np.arange(min_regulization_strength-1,min_regulization_strength+1), \n",
    "               'penalty':[best_penality],\n",
    "              'solver':[best_solver],\n",
    "              'max_iter': np.arange(min_iter-400,min_iter+400)\n",
    "}\n",
    "\n",
    "logreg =  LogisticRegression()\n",
    "grid_search = GridSearchCV(estimator = logreg, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1, # n_jobs=-1 will utilize all available CPUs \n",
    "                return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X,y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestlogreg = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d917668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9788000 Precision=1.0000000 Recall=0.6490066 F1=0.7871486\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "c_matrix = confusion_matrix(y, grid_search.predict(X))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0879f5c7",
   "metadata": {},
   "source": [
    "# By fitting Logistic Regression, SVM (with various kernels), and Decision trees According to my point of view, Logistic Regression using grid search is the best fit model as campared to other models woth the values of recall = 0.649125683060109"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
