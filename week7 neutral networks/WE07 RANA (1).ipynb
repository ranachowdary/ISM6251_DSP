{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42f1fa79",
   "metadata": {},
   "source": [
    "# SHANMUKA RANA PRATHAP CHOWDARY PONNAGANTI U97674115"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08933ec8",
   "metadata": {},
   "source": [
    "### Import a number of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0da1be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "np.random.seed(1) # set this to ensure the results are repeatable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850ad08",
   "metadata": {},
   "source": [
    "### Load the given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "010da3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('Loan_Data_X_train.csv')\n",
    "y_train = pd.read_csv('Loan_Data_y_train.csv')\n",
    "X_test = pd.read_csv('Loan_Data_X_test.csv')\n",
    "y_test = pd.read_csv('Loan_Data_y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1277fc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.387848</td>\n",
       "      <td>0.763656</td>\n",
       "      <td>-0.732099</td>\n",
       "      <td>1.875044</td>\n",
       "      <td>-0.437393</td>\n",
       "      <td>-0.501334</td>\n",
       "      <td>0.278657</td>\n",
       "      <td>0.413898</td>\n",
       "      <td>0.304375</td>\n",
       "      <td>-2.479919</td>\n",
       "      <td>1.206512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.387848</td>\n",
       "      <td>0.763656</td>\n",
       "      <td>-0.732099</td>\n",
       "      <td>1.875044</td>\n",
       "      <td>3.512960</td>\n",
       "      <td>-0.428032</td>\n",
       "      <td>0.451038</td>\n",
       "      <td>0.107065</td>\n",
       "      <td>0.304375</td>\n",
       "      <td>0.403239</td>\n",
       "      <td>-1.349506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.387848</td>\n",
       "      <td>-1.283896</td>\n",
       "      <td>-0.732099</td>\n",
       "      <td>-0.533321</td>\n",
       "      <td>-0.437393</td>\n",
       "      <td>-0.566972</td>\n",
       "      <td>0.232088</td>\n",
       "      <td>-0.140763</td>\n",
       "      <td>0.304375</td>\n",
       "      <td>0.403239</td>\n",
       "      <td>-0.071497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.387848</td>\n",
       "      <td>0.763656</td>\n",
       "      <td>-0.732099</td>\n",
       "      <td>1.875044</td>\n",
       "      <td>-0.437393</td>\n",
       "      <td>-0.477011</td>\n",
       "      <td>0.040931</td>\n",
       "      <td>-0.459398</td>\n",
       "      <td>0.304375</td>\n",
       "      <td>0.403239</td>\n",
       "      <td>-1.349506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.387848</td>\n",
       "      <td>-1.283896</td>\n",
       "      <td>-0.732099</td>\n",
       "      <td>1.875044</td>\n",
       "      <td>-0.437393</td>\n",
       "      <td>0.219858</td>\n",
       "      <td>-0.597514</td>\n",
       "      <td>-0.187968</td>\n",
       "      <td>0.304375</td>\n",
       "      <td>0.403239</td>\n",
       "      <td>-0.071497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.387848  0.763656 -0.732099  1.875044 -0.437393 -0.501334  0.278657   \n",
       "1  0.387848  0.763656 -0.732099  1.875044  3.512960 -0.428032  0.451038   \n",
       "2  0.387848 -1.283896 -0.732099 -0.533321 -0.437393 -0.566972  0.232088   \n",
       "3  0.387848  0.763656 -0.732099  1.875044 -0.437393 -0.477011  0.040931   \n",
       "4  0.387848 -1.283896 -0.732099  1.875044 -0.437393  0.219858 -0.597514   \n",
       "\n",
       "          7         8         9        10  \n",
       "0  0.413898  0.304375 -2.479919  1.206512  \n",
       "1  0.107065  0.304375  0.403239 -1.349506  \n",
       "2 -0.140763  0.304375  0.403239 -0.071497  \n",
       "3 -0.459398  0.304375  0.403239 -1.349506  \n",
       "4 -0.187968  0.304375  0.403239 -0.071497  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a5a736",
   "metadata": {},
   "source": [
    "# Logistic Regression using RandomSearch and Grid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84911d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best accuracy score is 0.702038567493113\n",
      "... with parameters: {'solver': 'liblinear', 'penalty': 'l2', 'max_iter': 979, 'C': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "910 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "280 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "340 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1471, in fit\n",
      "    raise ValueError(\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "290 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 464, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.70034435 0.68878788        nan 0.69044077 0.5        0.69044077\n",
      " 0.69044077 0.68878788 0.69044077        nan        nan        nan\n",
      "        nan 0.70201102        nan 0.5               nan        nan\n",
      " 0.70034435        nan 0.70201102 0.69209366        nan 0.70201102\n",
      " 0.69044077 0.69209366 0.5        0.70034435        nan 0.5\n",
      "        nan 0.69209366 0.69044077 0.69044077 0.70201102        nan\n",
      " 0.68878788 0.70034435 0.70034435 0.5               nan 0.68878788\n",
      "        nan 0.70034435        nan        nan        nan 0.70201102\n",
      "        nan        nan        nan 0.5        0.70034435        nan\n",
      "        nan 0.70203857        nan 0.70201102        nan        nan\n",
      " 0.70034435 0.69044077        nan        nan 0.69044077 0.69044077\n",
      " 0.70034435 0.69044077 0.69044077 0.70203857 0.69044077 0.50165289\n",
      "        nan        nan 0.68878788 0.70034435 0.70034435 0.50165289\n",
      "        nan 0.69044077 0.70201102        nan 0.69209366        nan\n",
      "        nan        nan 0.70201102 0.69044077 0.69209366 0.69044077\n",
      " 0.68878788 0.5        0.68878788        nan        nan        nan\n",
      "        nan        nan 0.69044077 0.69044077        nan 0.69044077\n",
      "        nan 0.69044077 0.70034435 0.68878788 0.69044077 0.49834711\n",
      " 0.69209366        nan        nan 0.68878788        nan        nan\n",
      "        nan 0.70034435 0.69209366 0.70201102 0.70034435        nan\n",
      "        nan        nan 0.69044077 0.69044077        nan        nan\n",
      " 0.69209366 0.69044077        nan 0.68878788 0.5        0.70034435\n",
      " 0.70201102 0.69044077        nan 0.69044077        nan        nan\n",
      " 0.69044077        nan        nan 0.68878788 0.69209366 0.69044077\n",
      " 0.50165289        nan        nan 0.69044077        nan 0.69044077\n",
      " 0.70034435 0.70034435 0.69044077        nan 0.69044077 0.69044077\n",
      " 0.5        0.69044077 0.69209366        nan 0.69044077 0.69209366\n",
      "        nan        nan 0.70034435        nan 0.69044077 0.69044077\n",
      " 0.69209366 0.68878788 0.70203857        nan        nan 0.69044077\n",
      " 0.70034435        nan        nan        nan 0.68878788 0.69044077\n",
      "        nan 0.69871901 0.69044077 0.69044077 0.69871901 0.69044077\n",
      "        nan 0.69209366        nan        nan 0.5               nan\n",
      "        nan 0.70034435 0.69209366 0.69209366 0.50165289        nan\n",
      " 0.68878788 0.69044077 0.69044077 0.69871901 0.69044077 0.70034435\n",
      " 0.5               nan        nan 0.69209366        nan        nan\n",
      " 0.70034435 0.69044077 0.69044077 0.69044077        nan        nan\n",
      " 0.68878788 0.69209366        nan 0.68878788 0.70034435 0.69044077\n",
      " 0.69044077 0.69044077 0.69044077 0.68878788        nan 0.49834711\n",
      "        nan 0.69044077 0.69209366        nan 0.69871901 0.69044077\n",
      " 0.69044077        nan 0.70203857 0.70034435 0.70034435 0.69871901\n",
      "        nan 0.69044077 0.69044077        nan 0.69044077        nan\n",
      " 0.69044077        nan 0.70201102 0.69044077 0.70034435 0.69044077\n",
      " 0.69209366 0.5               nan 0.69044077 0.69209366 0.69044077\n",
      "        nan 0.49669421 0.69044077 0.69044077 0.70201102        nan\n",
      " 0.5        0.70034435 0.70034435        nan 0.68878788 0.70201102\n",
      " 0.70034435        nan 0.70201102 0.69044077 0.70034435 0.70034435\n",
      " 0.68878788        nan 0.70034435 0.70034435 0.69209366 0.70034435\n",
      " 0.69044077        nan        nan        nan        nan 0.69044077\n",
      " 0.69871901        nan 0.69044077        nan 0.69871901 0.49834711\n",
      " 0.68878788 0.69044077 0.70034435 0.69044077 0.70201102        nan\n",
      " 0.69871901        nan 0.70203857        nan 0.69044077        nan\n",
      " 0.69044077        nan        nan        nan 0.68878788        nan\n",
      " 0.68878788 0.70034435        nan 0.5               nan        nan\n",
      " 0.69044077 0.69044077        nan 0.70034435 0.69044077 0.69044077\n",
      " 0.70201102 0.49834711 0.68878788        nan 0.69044077 0.70203857\n",
      " 0.69044077        nan        nan 0.70034435 0.68878788 0.68878788\n",
      "        nan 0.69044077 0.70201102 0.68878788 0.69044077 0.69871901\n",
      " 0.70201102        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.70034435 0.70034435 0.69209366 0.69209366\n",
      " 0.5        0.69044077        nan 0.70201102 0.68878788        nan\n",
      "        nan 0.69044077 0.69044077 0.70201102 0.69044077        nan\n",
      "        nan        nan 0.5        0.69044077 0.69044077 0.70201102\n",
      " 0.69044077 0.69209366        nan 0.69871901        nan 0.69044077\n",
      " 0.69209366 0.69044077        nan        nan 0.69044077        nan\n",
      " 0.69871901 0.68878788        nan 0.69209366 0.69044077 0.69044077\n",
      " 0.70034435        nan 0.70201102 0.70034435 0.70034435        nan\n",
      " 0.70034435 0.69044077        nan        nan 0.69209366 0.69044077\n",
      " 0.70201102 0.70203857 0.68878788        nan 0.68878788 0.69044077\n",
      " 0.69044077        nan 0.70201102        nan 0.69209366 0.70034435\n",
      " 0.69044077        nan 0.68878788 0.69044077        nan        nan\n",
      " 0.68878788        nan        nan 0.69044077        nan 0.68878788\n",
      " 0.70203857 0.68878788        nan 0.49834711 0.69044077 0.68878788\n",
      " 0.68878788 0.69044077        nan 0.69044077        nan 0.69044077\n",
      " 0.70034435 0.69044077 0.69044077 0.70034435 0.70034435 0.70201102\n",
      " 0.69044077        nan 0.69209366        nan 0.70034435 0.69209366\n",
      " 0.69044077 0.69871901        nan        nan 0.69044077        nan\n",
      " 0.69209366        nan 0.70201102        nan        nan 0.69044077\n",
      " 0.69044077        nan 0.69044077 0.70201102 0.69044077        nan\n",
      " 0.5        0.69044077        nan 0.68878788        nan        nan\n",
      " 0.5               nan 0.69044077 0.68878788 0.68878788 0.5\n",
      "        nan        nan        nan 0.69044077        nan        nan\n",
      "        nan 0.70201102        nan        nan        nan 0.70034435\n",
      "        nan        nan 0.69209366 0.70201102        nan 0.70034435\n",
      " 0.69044077 0.70034435]\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.70612905 0.71895437        nan 0.71854114 0.5        0.72102647\n",
      " 0.72144055 0.71812706 0.71854114        nan        nan        nan\n",
      "        nan 0.71482128        nan 0.5               nan        nan\n",
      " 0.70944168        nan 0.71482128 0.71978423        nan 0.71730575\n",
      " 0.71854114 0.71978423 0.5        0.70033195        nan 0.5\n",
      "        nan 0.71895608 0.72144055 0.72144055 0.71730575        nan\n",
      " 0.71895437 0.70033195 0.70944168 0.5               nan 0.71895437\n",
      "        nan 0.70033195        nan        nan        nan 0.71482128\n",
      "        nan        nan        nan 0.5        0.70033195        nan\n",
      "        nan 0.71688911        nan 0.71730575        nan        nan\n",
      " 0.70033195 0.71854114        nan        nan 0.71854114 0.71854114\n",
      " 0.70612905 0.71854114 0.71854114 0.71688911 0.71854114 0.49958592\n",
      "        nan        nan 0.71812706 0.70944168 0.70033195 0.49958592\n",
      "        nan 0.71854114 0.71482128        nan 0.71978423        nan\n",
      "        nan        nan 0.71730575 0.71854114 0.71895608 0.71854114\n",
      " 0.71812706 0.5        0.71812706        nan        nan        nan\n",
      "        nan        nan 0.72102647 0.71854114        nan 0.72144055\n",
      "        nan 0.72102647 0.70612905 0.71812706 0.71854114 0.50041408\n",
      " 0.71895608        nan        nan 0.71812706        nan        nan\n",
      "        nan 0.70033195 0.71978423 0.71730575 0.70033195        nan\n",
      "        nan        nan 0.71854114 0.71854114        nan        nan\n",
      " 0.71895608 0.71854114        nan 0.71895437 0.5        0.70033195\n",
      " 0.71482128 0.71854114        nan 0.71854114        nan        nan\n",
      " 0.71854114        nan        nan 0.71812706 0.71895608 0.71854114\n",
      " 0.49958592        nan        nan 0.71854114        nan 0.71854114\n",
      " 0.70612905 0.70612905 0.72102647        nan 0.71854114 0.71854114\n",
      " 0.5        0.72102647 0.71978423        nan 0.71854114 0.71978423\n",
      "        nan        nan 0.70033195        nan 0.71854114 0.71854114\n",
      " 0.71895608 0.71812706 0.71688911        nan        nan 0.71854114\n",
      " 0.70612905        nan        nan        nan 0.71812706 0.71854114\n",
      "        nan 0.70778365 0.72144055 0.71854114 0.70778365 0.71854114\n",
      "        nan 0.71978423        nan        nan 0.5               nan\n",
      "        nan 0.70944168 0.71895608 0.71978423 0.49958592        nan\n",
      " 0.71895437 0.71978252 0.71978252 0.70778365 0.72102647 0.70033195\n",
      " 0.5               nan        nan 0.71978423        nan        nan\n",
      " 0.70033195 0.71854114 0.72102647 0.71854114        nan        nan\n",
      " 0.71895437 0.71895608        nan 0.71895437 0.70612905 0.71978252\n",
      " 0.71854114 0.71854114 0.71854114 0.71812706        nan 0.50041408\n",
      "        nan 0.71854114 0.71895608        nan 0.70778365 0.71854114\n",
      " 0.71854114        nan 0.71688911 0.70033195 0.70033195 0.70778365\n",
      "        nan 0.71854114 0.72102647        nan 0.71978252        nan\n",
      " 0.71854114        nan 0.71482128 0.71854114 0.70033195 0.71854114\n",
      " 0.71978423 0.5               nan 0.71854114 0.71895608 0.71854114\n",
      "        nan 0.50082816 0.72102647 0.71854114 0.71482128        nan\n",
      " 0.5        0.70033195 0.70944168        nan 0.71812706 0.71730575\n",
      " 0.70612905        nan 0.71730575 0.71854114 0.70612905 0.70612905\n",
      " 0.71895437        nan 0.70612905 0.70944168 0.71895608 0.70944168\n",
      " 0.71854114        nan        nan        nan        nan 0.72102647\n",
      " 0.70778365        nan 0.71854114        nan 0.70778365 0.50041408\n",
      " 0.71895437 0.71854114 0.70944168 0.71854114 0.71482128        nan\n",
      " 0.70778365        nan 0.71688911        nan 0.71854114        nan\n",
      " 0.72144055        nan        nan        nan 0.71812706        nan\n",
      " 0.71895437 0.70612905        nan 0.5               nan        nan\n",
      " 0.71978252 0.71854114        nan 0.70033195 0.71854114 0.72102647\n",
      " 0.71482128 0.50041408 0.71895437        nan 0.71854114 0.71688911\n",
      " 0.72102647        nan        nan 0.70612905 0.71812706 0.71812706\n",
      "        nan 0.71854114 0.71730575 0.71812706 0.71978252 0.70778365\n",
      " 0.71730575        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.70033195 0.70944168 0.71978423 0.71895608\n",
      " 0.5        0.72102647        nan 0.71482128 0.71812706        nan\n",
      "        nan 0.71978252 0.71854114 0.71730575 0.71854114        nan\n",
      "        nan        nan 0.5        0.72102647 0.71978252 0.71482128\n",
      " 0.71854114 0.71978423        nan 0.70778365        nan 0.72144055\n",
      " 0.71978423 0.71854114        nan        nan 0.72144055        nan\n",
      " 0.70778365 0.71812706        nan 0.71978423 0.71854114 0.71854114\n",
      " 0.70033195        nan 0.71482128 0.70033195 0.70033195        nan\n",
      " 0.70033195 0.71854114        nan        nan 0.71895608 0.72102647\n",
      " 0.71730575 0.71688911 0.71895437        nan 0.71812706 0.71854114\n",
      " 0.71978252        nan 0.71482128        nan 0.71978423 0.70033195\n",
      " 0.71854114        nan 0.71895437 0.72144055        nan        nan\n",
      " 0.71812706        nan        nan 0.72102647        nan 0.71812706\n",
      " 0.71688911 0.71895437        nan 0.50041408 0.72144055 0.71895437\n",
      " 0.71812706 0.71854114        nan 0.71854114        nan 0.71854114\n",
      " 0.70944168 0.71854114 0.71854114 0.70944168 0.70612905 0.71482128\n",
      " 0.71854114        nan 0.71895608        nan 0.70612905 0.71978423\n",
      " 0.72144055 0.70778365        nan        nan 0.71854114        nan\n",
      " 0.71978423        nan 0.71482128        nan        nan 0.71978252\n",
      " 0.71854114        nan 0.72102647 0.71730575 0.72102647        nan\n",
      " 0.5        0.72102647        nan 0.71812706        nan        nan\n",
      " 0.5               nan 0.71854114 0.71812706 0.71895437 0.5\n",
      "        nan        nan        nan 0.72144055        nan        nan\n",
      "        nan 0.71730575        nan        nan        nan 0.70033195\n",
      "        nan        nan 0.71895608 0.71482128        nan 0.70612905\n",
      " 0.72102647 0.70944168]\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {'C':[0.001,0.01,0.1,1,10], # C is the regulization strength\n",
    "               'penalty':['l1', 'l2','elasticnet','none'],\n",
    "              'solver':['saga','liblinear'],\n",
    "              'max_iter': np.arange(500,1000)\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "lr = LogisticRegression()\n",
    "rand_search = RandomizedSearchCV(estimator = lr, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train,y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestlr = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4e35abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.7837838 Precision=0.7631579 Recall=0.9666667 F1=0.8529412\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "c_matrix = confusion_matrix(y_test, rand_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e99214d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1600 candidates, totalling 8000 fits\n",
      "The best accuracy score is 0.702038567493113\n",
      "... with parameters: {'C': 0.0010000000000000009, 'max_iter': 579, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "4000 fits failed out of a total of 8000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "4000 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1464, in fit\n",
      "    raise ValueError(\"Penalty term must be positive; got (C=%r)\" % self.C)\n",
      "ValueError: Penalty term must be positive; got (C=-0.999)\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan ... 0.70203857 0.70203857 0.70203857]\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [       nan        nan        nan ... 0.71688911 0.71688911 0.71688911]\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "best_penality = rand_search.best_params_['penalty']\n",
    "best_solver = rand_search.best_params_['solver']\n",
    "min_regulization_strength=rand_search.best_params_['C']\n",
    "min_iter = rand_search.best_params_['max_iter']\n",
    "\n",
    "#Using the best parameters from the Random Search to use as range for the parameters to do the grid search\n",
    "param_grid = {\n",
    "    \n",
    "    'C':np.arange(min_regulization_strength-1,min_regulization_strength+1), \n",
    "               'penalty':[best_penality],\n",
    "              'solver':[best_solver],\n",
    "              'max_iter': np.arange(min_iter-400,min_iter+400)\n",
    "}\n",
    "\n",
    "logreg =  LogisticRegression()\n",
    "grid_search = GridSearchCV(estimator = logreg, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1, # n_jobs=-1 will utilize all available CPUs \n",
    "                return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train,y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestlogreg = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8f7873f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.7837838 Precision=0.7631579 Recall=0.9666667 F1=0.8529412\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12d808d",
   "metadata": {},
   "source": [
    "## So, we have accurately recognized 70.20% of the loan status in our predictions, which are around 70.20% accurate by using logistic regression using both random and grid searchs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f499576b",
   "metadata": {},
   "source": [
    "# DecisionTrees using RandomSearch and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3913c7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9072 candidates, totalling 45360 fits\n",
      "The best accuracy score is 0.7715702479338843\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 15, 'max_leaf_nodes': 162, 'min_impurity_decrease': 0.0048, 'min_samples_leaf': 9, 'min_samples_split': 30}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(30,36),  \n",
    "    'min_samples_leaf': np.arange(6,12),\n",
    "    'min_impurity_decrease': np.arange(0.0048, 0.0054, 0.0001),\n",
    "    'max_leaf_nodes': np.arange(162,168), \n",
    "    'max_depth': np.arange(15,21), \n",
    "    'criterion': ['entropy'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e72af78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.7081081 Precision=0.7704918 Recall=0.7833333 F1=0.7768595\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de82698",
   "metadata": {},
   "source": [
    "## So, we have accurately recognized 77.15% of the loan status in our predictions, which are around 77.15% accurate by using DecisionTreeClassifier using grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec1736d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best accuracy score is 0.8344490358126722\n",
      "... with parameters: {'min_samples_split': 2, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0036, 'max_leaf_nodes': 155, 'max_depth': 40, 'criterion': 'entropy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "50 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.70690083 0.69699725 0.74342975 0.70034435 0.69865014 0.71690083\n",
      " 0.68046832 0.69369146 0.70030303 0.71858127 0.69534435 0.7119146\n",
      " 0.74673554 0.71358127 0.70863636 0.69033058 0.76662534 0.69865014\n",
      " 0.75823691 0.69865014 0.7335124  0.70201102 0.70195592 0.69367769\n",
      " 0.70530303 0.69699725 0.71523416 0.76988981 0.70034435 0.69699725\n",
      " 0.71192837 0.68873278 0.68544077 0.74174931 0.72515152 0.73012397\n",
      " 0.72681818 0.70034435 0.71524793 0.69033058 0.72187328 0.69865014\n",
      " 0.74836088 0.7069697  0.71524793 0.7235124  0.76661157 0.69865014\n",
      " 0.71194215 0.73679063 0.70530303 0.72352617 0.6837741  0.68046832\n",
      " 0.69865014 0.70034435 0.69865014 0.78318182 0.71852617 0.75833333\n",
      " 0.70034435 0.69367769 0.75338843 0.71853994 0.70195592 0.70366391\n",
      " 0.70195592 0.68046832 0.69865014 0.71358127 0.69865014 0.72847107\n",
      " 0.74836088 0.75006887 0.68046832 0.68046832 0.69699725 0.73017906\n",
      " 0.69699725 0.73847107 0.69873278 0.66559229 0.69866391 0.70034435\n",
      " 0.74011019 0.69033058 0.68713499 0.70034435 0.69701102 0.71192837\n",
      " 0.73681818 0.72352617 0.70034435 0.72353994 0.7434022         nan\n",
      " 0.71858127 0.74176309 0.74011019 0.7003168  0.69367769 0.73679063\n",
      " 0.69534435 0.68046832 0.69533058        nan 0.6837741  0.70195592\n",
      " 0.72020661 0.70862259        nan 0.73676309 0.71358127 0.75168044\n",
      " 0.73670799 0.73506887 0.69865014 0.70034435 0.70698347 0.73515152\n",
      " 0.71690083 0.70034435 0.71690083 0.74672176 0.72353994 0.68046832\n",
      " 0.7599449  0.70030303 0.75170799 0.71192837 0.73508264 0.69534435\n",
      " 0.69033058 0.70856749 0.75166667 0.69367769 0.76823691 0.68046832\n",
      " 0.75006887 0.68046832 0.7053168  0.70195592 0.70030303 0.73012397\n",
      " 0.7053168  0.70034435 0.69865014 0.70034435 0.69699725 0.69044077\n",
      " 0.70698347 0.69865014 0.74342975 0.7235124  0.7400551  0.71365014\n",
      " 0.68046832 0.74672176 0.70862259 0.69367769 0.72353994 0.69865014\n",
      " 0.74174931 0.69865014 0.68544077 0.70030303 0.68867769 0.69533058\n",
      " 0.69865014 0.73512397 0.76330579 0.70034435 0.74837466 0.75172176\n",
      " 0.69699725 0.73844353 0.69699725 0.76827824 0.73844353 0.69699725\n",
      " 0.70195592 0.70034435 0.755      0.75998623 0.70034435 0.70030303\n",
      " 0.69865014 0.71358127 0.74174931 0.71523416 0.69534435 0.70030303\n",
      " 0.70034435 0.70030303 0.72022039 0.72353994 0.69033058 0.6837741\n",
      " 0.7053168  0.71690083 0.69865014 0.70860882 0.70034435 0.77484848\n",
      " 0.71690083 0.75498623 0.70034435 0.70199725 0.71858127 0.72352617\n",
      " 0.73680441 0.70034435 0.75827824 0.75498623 0.70034435 0.68046832\n",
      " 0.74174931 0.67220386 0.70195592 0.69865014 0.76161157 0.69369146\n",
      " 0.69699725 0.75993113 0.68545455 0.70365014 0.69367769 0.71690083\n",
      " 0.76993113 0.73670799 0.69865014 0.71027548 0.74676309 0.71523416\n",
      " 0.71690083 0.71524793 0.70030303 0.75997245 0.70199725 0.69369146\n",
      " 0.71358127 0.69699725 0.68873278 0.71027548 0.6837741  0.70199725\n",
      " 0.7335124  0.69865014 0.67881543 0.69865014 0.69699725 0.69699725\n",
      " 0.70034435 0.75665289 0.70860882 0.69865014 0.70195592 0.69367769\n",
      " 0.68867769 0.76490358 0.69367769 0.68873278        nan 0.73677686\n",
      " 0.69865014 0.75662534 0.68213499 0.70695592 0.70698347 0.77820937\n",
      " 0.76161157 0.70030303 0.69865014 0.74009642 0.70366391 0.69865014\n",
      " 0.69865014 0.71524793 0.69865014 0.72849862 0.71523416 0.68873278\n",
      " 0.71523416 0.70360882 0.69865014 0.73348485 0.72516529 0.69367769\n",
      " 0.72515152 0.69534435 0.75829201 0.75002755 0.75170799 0.71690083\n",
      " 0.69046832 0.69865014 0.7069697  0.69865014        nan 0.72353994\n",
      " 0.70034435 0.74672176 0.72353994 0.70863636 0.70698347 0.7119697\n",
      " 0.70034435 0.70698347 0.69870523 0.69367769 0.77487603 0.72353994\n",
      " 0.76002755 0.69865014 0.7334022  0.68046832 0.69699725 0.71356749\n",
      " 0.6903719  0.74512397 0.70034435 0.69865014 0.69703857 0.70034435\n",
      " 0.69534435 0.69869146 0.70860882 0.71690083 0.75001377 0.69703857\n",
      " 0.73012397 0.69369146 0.69865014 0.70034435 0.69703857 0.69203857\n",
      " 0.77323691 0.74673554 0.71690083 0.7053168  0.71522039 0.72187328\n",
      " 0.68046832 0.69699725 0.69534435 0.72353994 0.69865014 0.69534435\n",
      " 0.75337466 0.69869146 0.68046832 0.69699725 0.69865014 0.69699725\n",
      " 0.70034435 0.70034435 0.74669421 0.71523416 0.83444904 0.74012397\n",
      "        nan 0.71358127 0.70860882 0.69703857 0.69865014 0.69865014\n",
      " 0.72353994 0.70195592 0.68544077 0.69705234 0.69534435 0.75830579\n",
      " 0.69535813 0.69869146 0.72020661 0.76993113 0.69367769 0.70856749\n",
      " 0.68046832 0.70695592 0.69869146 0.69865014 0.69699725 0.71690083\n",
      " 0.71852617 0.68702479 0.69534435 0.69534435 0.75004132 0.72356749\n",
      " 0.69699725 0.69869146 0.69865014 0.72519284 0.74174931 0.70034435\n",
      " 0.70034435 0.76163912 0.69699725 0.71522039 0.69038567 0.68046832\n",
      " 0.69865014 0.74837466 0.69699725 0.71688705 0.72187328 0.72516529\n",
      " 0.69046832 0.70034435        nan 0.70030303 0.73184573 0.71523416\n",
      " 0.69865014 0.71194215 0.70030303 0.73513774 0.69865014 0.74670799\n",
      " 0.69865014 0.68046832 0.69701102 0.76166667 0.69367769 0.74344353\n",
      " 0.69369146 0.72352617 0.70199725 0.71522039 0.68046832 0.76497245\n",
      "        nan 0.72349862 0.74177686 0.68713499 0.68046832 0.69699725\n",
      " 0.70034435 0.7400551  0.7400551  0.69367769 0.71028926 0.69865014\n",
      " 0.70034435 0.69533058 0.74177686 0.69699725 0.71522039 0.69033058\n",
      " 0.70034435 0.70030303 0.70195592 0.71027548 0.75337466 0.69369146\n",
      " 0.69534435 0.70034435 0.69866391 0.73836088 0.69703857 0.71194215\n",
      "        nan 0.72849862 0.70034435 0.68046832 0.81949036 0.69033058\n",
      " 0.69865014 0.73842975 0.70030303 0.74841598 0.68873278 0.69033058\n",
      " 0.69703857 0.70030303 0.70860882 0.69865014 0.70034435 0.71359504\n",
      " 0.69865014 0.74011019 0.75663912 0.71690083 0.69869146 0.79632231\n",
      " 0.75337466 0.71524793 0.69367769 0.74508264 0.70360882 0.73011019\n",
      " 0.72353994 0.71524793        nan 0.77820937 0.69699725 0.74506887\n",
      " 0.69865014 0.71690083]\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.70943655 0.72392844 0.79760536 0.70033195 0.71688911 0.75911658\n",
      " 0.711092   0.71440463 0.72144397 0.78105333 0.71647503 0.75994473\n",
      " 0.78518471 0.75249217 0.750835   0.71275003 0.84851736 0.72392844\n",
      " 0.85430676 0.71688911 0.79097753 0.72184864 0.72517068 0.70239806\n",
      " 0.72971784 0.72392844 0.74876546 0.84436973 0.70033195 0.71357648\n",
      " 0.75828927 0.71730318 0.72848074 0.81043666 0.78230413 0.76822288\n",
      " 0.76656657 0.70033195 0.7640821  0.71275003 0.77525367 0.71688911\n",
      " 0.81043923 0.73013021 0.7640821  0.77277005 0.85927998 0.72392844\n",
      " 0.7632548  0.78808839 0.72971784 0.7764959  0.71399055 0.711092\n",
      " 0.71688911 0.70033195 0.71688911 0.86299899 0.73883442 0.80464127\n",
      " 0.70033195 0.70239806 0.79718871 0.76615506 0.72392844 0.74462297\n",
      " 0.72599884 0.711092   0.71688911 0.74752323 0.71688911 0.79139076\n",
      " 0.83195421 0.79884417 0.711092   0.711092   0.72185805 0.77525195\n",
      " 0.71357648 0.8071266  0.74751724 0.72765515 0.73427784 0.70033195\n",
      " 0.79760621 0.71275003 0.74710744 0.70033195 0.74545797 0.75828927\n",
      " 0.80712404 0.7764959  0.70033195 0.77111716 0.78270537        nan\n",
      " 0.7661465  0.80050477 0.79884845 0.73800455 0.70239806 0.7938855\n",
      " 0.70653714 0.711092   0.72020259        nan 0.70695122 0.73510857\n",
      " 0.77235683 0.72516127        nan 0.78104649 0.7458652  0.81830159\n",
      " 0.81004312 0.79017162 0.71688911 0.70033195 0.758285   0.79429359\n",
      " 0.76491111 0.70033195 0.76532433 0.78353524 0.77111716 0.711092\n",
      " 0.82658146 0.7305537  0.82367948 0.75828927 0.79885016 0.70653714\n",
      " 0.71275003 0.70984977 0.81416851 0.70239806 0.81664699 0.711092\n",
      " 0.7963597  0.711092   0.7574594  0.72517068 0.72848331 0.76822288\n",
      " 0.75621802 0.70033195 0.71688911 0.70033195 0.72392844 0.72433311\n",
      " 0.75952723 0.71688911 0.8029841  0.76905104 0.77980768 0.77649419\n",
      " 0.711092   0.83817053 0.72516127 0.70239806 0.77111716 0.72392844\n",
      " 0.81126311 0.71688911 0.72433996 0.72889739 0.70571069 0.71316325\n",
      " 0.71688911 0.78063327 0.85389696 0.70033195 0.78063669 0.80091371\n",
      " 0.72392844 0.79967404 0.72185805 0.85099328 0.79926766 0.71357648\n",
      " 0.70819345 0.70033195 0.79056517 0.80546943 0.70033195 0.72061581\n",
      " 0.71688911 0.771124   0.78974471 0.74503876 0.70653714 0.72144397\n",
      " 0.70033195 0.73013962 0.7764959  0.77111716 0.71275003 0.72724107\n",
      " 0.77029584 0.75911658 0.72641292 0.73676403 0.70033195 0.85969834\n",
      " 0.76532433 0.82119929 0.70033195 0.7529054  0.76614821 0.77608182\n",
      " 0.79346372 0.70033195 0.84478552 0.81457488 0.70033195 0.711092\n",
      " 0.78229215 0.71192016 0.72517068 0.71688911 0.81664528 0.72144397\n",
      " 0.71357648 0.84685591 0.74338415 0.71646733 0.70571069 0.76491111\n",
      " 0.81002173 0.81004483 0.71688911 0.74089797 0.79428845 0.74545112\n",
      " 0.76491111 0.7640821  0.73013962 0.79139332 0.74461954 0.71440463\n",
      " 0.75249217 0.72392844 0.71730318 0.74089797 0.72724107 0.7529054\n",
      " 0.80174358 0.71688911 0.71688911 0.71688911 0.72392844 0.71357648\n",
      " 0.70033195 0.83899013 0.7363508  0.71688911 0.72517068 0.70239806\n",
      " 0.70571069 0.83899269 0.70571069 0.71730318        nan 0.80588522\n",
      " 0.71688911 0.84520045 0.72765258 0.73552179 0.758285   0.86134353\n",
      " 0.85389696 0.73013962 0.71688911 0.81002515 0.75042007 0.72434252\n",
      " 0.71688911 0.75580395 0.71688911 0.77069452 0.74338244 0.73096778\n",
      " 0.74876546 0.72599884 0.72392844 0.78146314 0.76822288 0.70612392\n",
      " 0.77650702 0.70322451 0.83237856 0.81580429 0.79346628 0.76491111\n",
      " 0.7483454  0.71688911 0.75124994 0.71688911        nan 0.77111716\n",
      " 0.70033195 0.80381739 0.77111716 0.750835   0.75787092 0.75331006\n",
      " 0.70033195 0.75911315 0.74296665 0.70239806 0.84106651 0.77111716\n",
      " 0.80877778 0.71688911 0.79472392 0.711092   0.72392844 0.76449019\n",
      " 0.70860924 0.85512807 0.70033195 0.71688911 0.70654313 0.70033195\n",
      " 0.71647503 0.70364458 0.7363508  0.76491025 0.82988553 0.70654313\n",
      " 0.76822288 0.71440463 0.71688911 0.70033195 0.70612905 0.7384212\n",
      " 0.85389268 0.80133207 0.76532433 0.75621802 0.76201513 0.77525367\n",
      " 0.711092   0.72392844 0.70653714 0.77111716 0.72434252 0.70653714\n",
      " 0.7827028  0.71068391 0.711092   0.72185805 0.71688911 0.71978766\n",
      " 0.70033195 0.70033195 0.81871482 0.74503876 0.96068135 0.79429273\n",
      "        nan 0.75249217 0.73676403 0.70612905 0.72392844 0.71688911\n",
      " 0.77111716 0.72517068 0.72848074 0.7206064  0.70653714 0.83154099\n",
      " 0.74380165 0.71068391 0.77235683 0.80919357 0.70571069 0.70984977\n",
      " 0.711092   0.75538987 0.70364458 0.71688911 0.72392844 0.76491025\n",
      " 0.76035881 0.7173049  0.70653714 0.70653714 0.78890885 0.7715261\n",
      " 0.72392844 0.70364458 0.71688911 0.78602228 0.78229215 0.70033195\n",
      " 0.70033195 0.83485447 0.72061581 0.76201513 0.72682699 0.711092\n",
      " 0.72392844 0.81581798 0.71978766 0.76366374 0.77525367 0.76325394\n",
      " 0.7483454  0.70033195        nan 0.72061581 0.78104307 0.74503876\n",
      " 0.71688911 0.75994217 0.72061581 0.78436169 0.71688911 0.82450251\n",
      " 0.71688911 0.711092   0.74545797 0.80712318 0.70571069 0.8294706\n",
      " 0.71440463 0.7764959  0.7529054  0.76201513 0.711092   0.85845097\n",
      "        nan 0.7789915  0.7893255  0.72557535 0.70777937 0.72392844\n",
      " 0.70033195 0.77980768 0.77980768 0.70571069 0.75787092 0.72392844\n",
      " 0.70033195 0.71316325 0.79553497 0.72061581 0.76201513 0.71275003\n",
      " 0.70033195 0.72144397 0.72599884 0.74420889 0.8331973  0.71440463\n",
      " 0.70653714 0.70033195 0.75001283 0.80300549 0.71316839 0.75952723\n",
      "        nan 0.76697637 0.70033195 0.711092   0.93626012 0.71275003\n",
      " 0.71688911 0.79057115 0.72848331 0.79553069 0.71730318 0.71275003\n",
      " 0.71316839 0.72061581 0.73676403 0.72434252 0.70033195 0.76573841\n",
      " 0.71688911 0.8178858  0.82161337 0.77029328 0.70364458 0.87127543\n",
      " 0.80174272 0.76739473 0.70612392 0.80339219 0.73469449 0.79224201\n",
      " 0.77111716 0.7640821         nan 0.85968722 0.72392844 0.78063669\n",
      " 0.71688911 0.76491111]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,55),  \n",
    "    'min_samples_leaf': np.arange(1,55),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 200), \n",
    "    'max_depth': np.arange(1,50), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22088b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.6702703 Precision=0.7610619 Recall=0.7166667 F1=0.7381974\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "c_matrix = confusion_matrix(y_test, rand_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a7e352",
   "metadata": {},
   "source": [
    "## So, we have accurately recognized 83.44% of the loan status in our predictions, which are around 83.44% accurate by using DecisionTreeClassifier using random search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4728026e",
   "metadata": {},
   "source": [
    "# Fitting a SVM classification model using polynomial kernal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f417410",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame({\"model\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24b7b71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_poly_model = SVC(kernel=\"poly\", degree=3, coef0=1, C=10)\n",
    "_ = svm_poly_model.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d438246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "model_preds = svm_poly_model.predict(X_test)\n",
    "c_matrix = confusion_matrix(y_test, model_preds)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"poly svm\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cf3a567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best accuracy score is 0.7946831955922865\n",
      "... with parameters: {'min_samples_split': 12, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.004600000000000001, 'max_leaf_nodes': 173, 'max_depth': 34, 'criterion': 'gini'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "40 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "40 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.7484022  0.76663912 0.70030303 0.74174931 0.70203857 0.72516529\n",
      " 0.70034435 0.70698347 0.72356749 0.70533058 0.70201102 0.75497245\n",
      " 0.69699725 0.70360882 0.70365014 0.69367769 0.69533058 0.69865014\n",
      " 0.70030303 0.71192837 0.70860882 0.73679063 0.73674931 0.74508264\n",
      " 0.68874656 0.72356749 0.70195592 0.76823691 0.68709366 0.75001377\n",
      " 0.68212121 0.72522039 0.68046832 0.71688705 0.68046832 0.72022039\n",
      " 0.78318182 0.70035813 0.78640496 0.69699725 0.75004132 0.70367769\n",
      " 0.71192837 0.71688705 0.75833333 0.69865014 0.76490358 0.72187328\n",
      " 0.70034435 0.70034435 0.69870523 0.73512397 0.7484022  0.7946832\n",
      " 0.71690083 0.75997245 0.71690083 0.70034435 0.70034435 0.69869146\n",
      " 0.70690083 0.70533058 0.73844353 0.69367769 0.76658402 0.69869146\n",
      " 0.7053168  0.68867769 0.70698347 0.69865014 0.72522039 0.70030303\n",
      " 0.68713499 0.69865014 0.71690083 0.71690083 0.68046832 0.68046832\n",
      " 0.70365014        nan 0.69699725 0.71852617 0.69203857 0.72020661\n",
      " 0.76495868 0.69699725 0.72355372 0.72852617 0.69865014 0.70698347\n",
      " 0.71523416 0.7003168  0.75827824 0.71523416 0.69534435 0.75166667\n",
      " 0.74670799 0.75004132 0.7434022  0.69369146 0.7334573  0.72517906\n",
      " 0.75172176 0.69865014 0.68046832 0.69044077 0.70034435 0.73341598\n",
      " 0.69699725 0.69369146 0.69866391 0.7053168  0.73015152 0.68544077\n",
      " 0.74176309 0.69369146 0.76162534 0.69865014 0.74506887 0.68867769\n",
      " 0.7649449  0.71192837 0.70034435 0.70034435 0.71524793 0.69865014\n",
      " 0.69866391        nan 0.71192837 0.69865014 0.75336088 0.73672176\n",
      " 0.71522039 0.70190083 0.70034435 0.72020661 0.70856749 0.69865014\n",
      " 0.74673554 0.7484022  0.69865014 0.69865014 0.69369146 0.70034435\n",
      " 0.71690083 0.69699725 0.70195592 0.68046832 0.68046832 0.69369146\n",
      " 0.74506887 0.76661157 0.75006887 0.70360882        nan 0.74998623\n",
      " 0.75829201 0.69699725 0.68046832 0.70030303 0.70698347 0.69699725\n",
      " 0.69033058 0.69699725 0.72355372 0.70034435 0.70698347 0.68873278\n",
      " 0.71852617 0.77652893 0.70034435 0.75001377 0.71852617 0.69865014\n",
      " 0.71523416 0.74179063 0.71852617 0.75004132 0.78811295 0.69369146\n",
      " 0.69535813 0.68046832 0.7384573  0.76990358 0.71853994 0.68046832\n",
      " 0.76827824 0.68046832 0.70030303 0.68046832 0.69533058 0.69869146\n",
      " 0.68046832 0.71523416        nan 0.71359504 0.71523416 0.75166667\n",
      " 0.71690083 0.75006887 0.74342975 0.71690083 0.69699725 0.71358127\n",
      " 0.69869146 0.75337466 0.71853994 0.71523416 0.70034435 0.71852617\n",
      " 0.71192837 0.70030303 0.75172176 0.70034435 0.75669421 0.74174931\n",
      " 0.73177686 0.71362259 0.69699725 0.69865014 0.69369146 0.71524793\n",
      " 0.70034435 0.69699725 0.72188705 0.69534435 0.71690083 0.7268595\n",
      "        nan 0.69865014 0.75831956 0.73341598 0.73842975 0.72352617\n",
      " 0.69699725 0.75337466 0.7053168  0.69865014 0.68867769 0.70360882\n",
      " 0.7284573         nan 0.72852617 0.70034435 0.69367769 0.69699725\n",
      " 0.75001377 0.68544077 0.70695592 0.72680441 0.70860882 0.7119146\n",
      " 0.73674931 0.69534435 0.75170799 0.69533058 0.75830579 0.69203857\n",
      " 0.68046832 0.69865014 0.72680441 0.69865014 0.69865014 0.71194215\n",
      " 0.74837466 0.69865014 0.78482094 0.69535813 0.69865014 0.71358127\n",
      " 0.69699725 0.72353994 0.69534435 0.69033058 0.76827824 0.71524793\n",
      " 0.73515152 0.69865014 0.71690083 0.72349862 0.71523416 0.70034435\n",
      " 0.68867769 0.70365014 0.69865014 0.69865014 0.70034435 0.70365014\n",
      " 0.73341598 0.69202479 0.69865014 0.71852617 0.73349862 0.71522039\n",
      " 0.69865014 0.70034435 0.71523416 0.68874656 0.71523416 0.77980716\n",
      " 0.69367769 0.7764876  0.69367769 0.68867769 0.70034435 0.69367769\n",
      " 0.69870523 0.67881543 0.74174931 0.69699725 0.69533058 0.69701102\n",
      " 0.75827824 0.69865014 0.68873278 0.68873278 0.68046832 0.70034435\n",
      " 0.73011019 0.73176309 0.71858127 0.71690083 0.70698347 0.7615978\n",
      " 0.68217631 0.77484848 0.77150138 0.73011019 0.70698347 0.69865014\n",
      " 0.68874656 0.71524793 0.70860882 0.69865014 0.69699725 0.74011019\n",
      " 0.68046832 0.73516529 0.69865014 0.76822314 0.7069697  0.72023416\n",
      " 0.73676309 0.70034435 0.69369146 0.69367769 0.69699725 0.71690083\n",
      " 0.75995868 0.68046832 0.70860882 0.69865014 0.68046832 0.71852617\n",
      " 0.70690083 0.69869146 0.70034435 0.70190083 0.69865014 0.75172176\n",
      " 0.69869146 0.69699725 0.70195592 0.73679063 0.75665289 0.69870523\n",
      " 0.69865014 0.74174931 0.75501377 0.70199725 0.69865014 0.71358127\n",
      " 0.71852617 0.6837741  0.69699725 0.73842975 0.68873278 0.71027548\n",
      " 0.73672176 0.70526171 0.69367769 0.70360882 0.75166667 0.7484022\n",
      " 0.70034435 0.71853994 0.71690083 0.68046832 0.69533058 0.70195592\n",
      " 0.69865014 0.72681818 0.69865014 0.70365014 0.68867769 0.69367769\n",
      " 0.69367769 0.73508264 0.69865014 0.71192837 0.69369146 0.74009642\n",
      " 0.70201102 0.69699725 0.73681818 0.75669421 0.73347107 0.72356749\n",
      " 0.7235124  0.68544077 0.70856749 0.74174931 0.70034435 0.72355372\n",
      " 0.69865014 0.75662534 0.71522039 0.70860882 0.69869146 0.68046832\n",
      " 0.69699725 0.71690083 0.71192837 0.69369146 0.71855372 0.7699449\n",
      " 0.76497245 0.69534435 0.74672176 0.75168044 0.76661157 0.69869146\n",
      " 0.68046832 0.68046832 0.73513774 0.71523416 0.70034435 0.70034435\n",
      " 0.66559229 0.73341598 0.70698347 0.69865014 0.71688705 0.71690083\n",
      " 0.73508264 0.69701102 0.70860882 0.74841598 0.71026171 0.68873278\n",
      " 0.70030303 0.69865014 0.72515152 0.70034435 0.70856749 0.68548209\n",
      " 0.67881543 0.72353994 0.69869146 0.69534435 0.71194215 0.68046832\n",
      " 0.70199725 0.71359504 0.74341598 0.79143251 0.69699725 0.69865014\n",
      " 0.7500551  0.68867769 0.69865014 0.69202479 0.69865014 0.71192837\n",
      " 0.76827824 0.71192837 0.75166667 0.69865014 0.69699725 0.70366391\n",
      " 0.69533058 0.7284573  0.68873278 0.78482094        nan 0.69865014\n",
      " 0.7053168  0.76495868 0.69865014 0.74009642 0.69865014 0.71192837\n",
      " 0.75834711 0.70533058 0.69367769 0.68212121 0.69033058 0.71027548\n",
      " 0.69699725        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.81458258 0.85017282 0.73013962 0.7851907  0.74833855 0.76780881\n",
      " 0.70033195 0.75745769 0.7715261  0.75290112 0.73385863 0.83237171\n",
      " 0.72392844 0.73469449 0.75580395 0.70571069 0.72020259 0.71688911\n",
      " 0.73013962 0.75828927 0.73676403 0.79140102 0.7793936  0.83981828\n",
      " 0.7293089  0.7735965  0.72517068 0.81788923 0.73303561 0.81458772\n",
      " 0.72558476 0.77318156 0.711092   0.75042007 0.711092   0.7764959\n",
      " 0.88038516 0.72226272 0.87127629 0.71357648 0.80629502 0.75869907\n",
      " 0.75828927 0.77442979 0.79346201 0.71688911 0.82823691 0.77525367\n",
      " 0.70033195 0.70033195 0.72226272 0.80216279 0.7827028  0.8791395\n",
      " 0.76491111 0.83196704 0.75911658 0.70033195 0.70033195 0.70364458\n",
      " 0.70943655 0.7636646  0.8071343  0.70571069 0.81167805 0.70364458\n",
      " 0.75663125 0.71937529 0.75787092 0.71688911 0.77856458 0.7305537\n",
      " 0.73344284 0.71688911 0.76491111 0.75911658 0.711092   0.711092\n",
      " 0.71646733        nan 0.72061581 0.76035881 0.71771726 0.76905104\n",
      " 0.83071625 0.71978766 0.77773557 0.77690913 0.72392844 0.75787092\n",
      " 0.74876546 0.72350752 0.8451996  0.74503876 0.70653714 0.7918151\n",
      " 0.8274002  0.78684274 0.77815051 0.71440463 0.78063327 0.77277005\n",
      " 0.81829988 0.71688911 0.711092   0.74379481 0.70033195 0.77485071\n",
      " 0.72392844 0.71440463 0.74173126 0.75621802 0.77691255 0.72433996\n",
      " 0.79594819 0.71440463 0.81581883 0.72641292 0.7947128  0.70571069\n",
      " 0.8079539  0.75663125 0.70033195 0.70033195 0.7640821  0.71688911\n",
      " 0.73427784        nan 0.75828927 0.71688911 0.83568092 0.79348425\n",
      " 0.76201513 0.70902332 0.70033195 0.76905104 0.70984977 0.72392844\n",
      " 0.78518471 0.7827028  0.71688911 0.71688911 0.71440463 0.70033195\n",
      " 0.76491111 0.72392844 0.72517068 0.711092   0.711092   0.71440463\n",
      " 0.78063669 0.83485105 0.7963597  0.72641292        nan 0.7769134\n",
      " 0.83361566 0.72185805 0.711092   0.72061581 0.75952723 0.71357648\n",
      " 0.71275003 0.72392844 0.77773557 0.70033195 0.75787092 0.71730318\n",
      " 0.73883442 0.88742108 0.70033195 0.79595589 0.76035881 0.71688911\n",
      " 0.74338244 0.80712318 0.76035881 0.80174871 0.8890791  0.71440463\n",
      " 0.74380165 0.711092   0.79760621 0.87292661 0.77235683 0.711092\n",
      " 0.81043581 0.711092   0.72889739 0.711092   0.71316325 0.70364458\n",
      " 0.711092   0.74876546        nan 0.76573841 0.74876546 0.7918151\n",
      " 0.75911658 0.7963597  0.80464213 0.75911658 0.71357648 0.75249217\n",
      " 0.70364458 0.7827028  0.77318755 0.77277347 0.73510087 0.76035881\n",
      " 0.75828927 0.72061581 0.82864928 0.70033195 0.83775217 0.78394761\n",
      " 0.77484814 0.75620861 0.71357648 0.71688911 0.72144397 0.76739473\n",
      " 0.70033195 0.71357648 0.77608439 0.71854542 0.76491111 0.78353182\n",
      "        nan 0.71688911 0.83858033 0.79265609 0.80712917 0.7764959\n",
      " 0.72392844 0.79677292 0.75663125 0.71688911 0.71937529 0.73469449\n",
      " 0.77525794        nan 0.77690913 0.70033195 0.7160618  0.72392844\n",
      " 0.8005022  0.72433996 0.73552179 0.7653269  0.7363508  0.75994473\n",
      " 0.7793936  0.70653714 0.7827028  0.70736701 0.83278665 0.71937358\n",
      " 0.711092   0.71688911 0.78354636 0.71688911 0.72434252 0.75497322\n",
      " 0.83692059 0.71688911 0.86713892 0.72765258 0.71688911 0.75249217\n",
      " 0.72392844 0.77111716 0.70653714 0.71275003 0.84395907 0.76739473\n",
      " 0.78808241 0.72392844 0.76491111 0.7789915  0.74503876 0.70033195\n",
      " 0.71937529 0.75580395 0.71688911 0.71688911 0.70033195 0.71646733\n",
      " 0.79141386 0.70529661 0.71688911 0.73883442 0.79677806 0.76201513\n",
      " 0.71688911 0.70033195 0.74876546 0.7293089  0.76781137 0.87293431\n",
      " 0.70612392 0.84685163 0.70239806 0.71937529 0.70033195 0.70571069\n",
      " 0.74296665 0.71481871 0.78518642 0.71357648 0.71316325 0.73800455\n",
      " 0.8447898  0.71688911 0.71730318 0.71730318 0.711092   0.70033195\n",
      " 0.79099978 0.77526479 0.75372414 0.75911658 0.75745769 0.82533836\n",
      " 0.73841179 0.84064986 0.84520131 0.78934346 0.75911315 0.72392844\n",
      " 0.72599627 0.76449618 0.76160105 0.71688911 0.71978766 0.79470766\n",
      " 0.711092   0.80008726 0.71688911 0.83361053 0.73013021 0.76573413\n",
      " 0.78104649 0.70033195 0.72144397 0.70239806 0.72185805 0.76491111\n",
      " 0.79097924 0.711092   0.76160105 0.71688911 0.711092   0.76035881\n",
      " 0.70943655 0.70364458 0.70033195 0.70902332 0.71688911 0.83029961\n",
      " 0.71068391 0.71357648 0.72517068 0.79140102 0.81912462 0.72226272\n",
      " 0.71688911 0.78312031 0.78932293 0.7326164  0.71688911 0.75249217\n",
      " 0.73883442 0.72724107 0.71978766 0.81291857 0.71730318 0.75704704\n",
      " 0.79348425 0.72102989 0.7160618  0.72641292 0.79429957 0.81291943\n",
      " 0.70033195 0.77235683 0.76491025 0.711092   0.71316325 0.72517068\n",
      " 0.71688911 0.78478004 0.71688911 0.71646733 0.71937529 0.70239806\n",
      " 0.70571069 0.77690314 0.75167171 0.76077203 0.71440463 0.80919785\n",
      " 0.72184864 0.71357648 0.80670996 0.83402289 0.81002601 0.7715261\n",
      " 0.76905104 0.72848074 0.70984977 0.78022175 0.70033195 0.77773557\n",
      " 0.71688911 0.84230105 0.76201513 0.73676403 0.70364458 0.711092\n",
      " 0.72061581 0.76491025 0.75124994 0.71440463 0.76449618 0.87004004\n",
      " 0.81043581 0.7442183  0.79595503 0.79968174 0.83485105 0.70364458\n",
      " 0.711092   0.711092   0.80049878 0.74876546 0.70033195 0.70033195\n",
      " 0.71647503 0.77278032 0.75869907 0.72392844 0.77526479 0.76491111\n",
      " 0.79512859 0.73800455 0.75828842 0.80753469 0.73800626 0.71730318\n",
      " 0.72061581 0.72434252 0.78189005 0.70033195 0.70984977 0.72019232\n",
      " 0.71481871 0.77111716 0.70364458 0.70653714 0.75994217 0.711092\n",
      " 0.7363431  0.76573841 0.83029961 0.89155844 0.72392844 0.72434252\n",
      " 0.81581798 0.71937529 0.72392844 0.70571069 0.71688911 0.75828927\n",
      " 0.85596821 0.75828927 0.7918151  0.71688911 0.71357648 0.72557535\n",
      " 0.71316325 0.78768715 0.71730318 0.88245299        nan 0.72392844\n",
      " 0.7574594  0.82739849 0.71688911 0.82534093 0.72434252 0.75828927\n",
      " 0.83071026 0.75745684 0.70239806 0.72558476 0.71275003 0.75704704\n",
      " 0.72061581        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,55),  \n",
    "    'min_samples_leaf': np.arange(1,55),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 200), \n",
    "    'max_depth': np.arange(1,50), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "svm = SVC()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0d1301",
   "metadata": {},
   "source": [
    "### So, we have accurately recognized 79.46% of the loan status in our predictions, which are around 79.46% accurate by using svm poly using random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62bbc098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9072 candidates, totalling 45360 fits\n",
      "The best accuracy score is 0.7715702479338843\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 15, 'max_leaf_nodes': 162, 'min_impurity_decrease': 0.0048, 'min_samples_leaf': 9, 'min_samples_split': 30}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(30,36),  \n",
    "    'min_samples_leaf': np.arange(6,12),\n",
    "    'min_impurity_decrease': np.arange(0.0048, 0.0054, 0.0001),\n",
    "    'max_leaf_nodes': np.arange(162,168), \n",
    "    'max_depth': np.arange(15,21), \n",
    "    'criterion': ['entropy'],\n",
    "}\n",
    "\n",
    "svm = SVC()\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a89c70",
   "metadata": {},
   "source": [
    "### So, we have accurately recognized 77.15% of the loan status in our predictions, which are around 77.15% accurate by using svm poly using grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493aebf4",
   "metadata": {},
   "source": [
    "# Fitting a SVM classification model using linear kernal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "347c2b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_lin_model = SVC(kernel=\"linear\")\n",
    "_ = svm_lin_model.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4da11119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "model_preds = svm_lin_model.predict(X_test)\n",
    "c_matrix = confusion_matrix(y_test, model_preds)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"linear svm\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed06e30",
   "metadata": {},
   "source": [
    "# Fitting a SVM classification model using rbf kernal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "503e35ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_rbf_model = SVC(kernel=\"rbf\", C=10, gamma='scale',probability=True)\n",
    "_ = svm_rbf_model.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d45d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "model_preds = svm_rbf_model.predict(X_test)\n",
    "c_matrix = confusion_matrix(y_test, model_preds)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"rbf svm\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f326bbb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>poly svm</td>\n",
       "      <td>0.713514</td>\n",
       "      <td>0.763780</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.785425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linear svm</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.756410</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.855072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rbf svm</td>\n",
       "      <td>0.718919</td>\n",
       "      <td>0.761538</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.792000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model  Accuracy  Precision    Recall        F1\n",
       "0    poly svm  0.713514   0.763780  0.808333  0.785425\n",
       "0  linear svm  0.783784   0.756410  0.983333  0.855072\n",
       "0     rbf svm  0.718919   0.761538  0.825000  0.792000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf9eaa2",
   "metadata": {},
   "source": [
    "### So, we have accurately recognized different SVM (with various kernels) of the loan status in our predictions, which are around 71.3%, 78.3%, 71.89% accurate by using polysvm, linear svm, rbf svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6356ae",
   "metadata": {},
   "source": [
    "### Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f91fda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ann = MLPClassifier(hidden_layer_sizes=(60,50,40), solver='adam', max_iter=200)\n",
    "_ = ann.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5be42f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.91 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = ann.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b48ba07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.55      0.58        65\n",
      "           1       0.77      0.80      0.78       120\n",
      "\n",
      "    accuracy                           0.71       185\n",
      "   macro avg       0.68      0.68      0.68       185\n",
      "weighted avg       0.71      0.71      0.71       185\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80a3d4d",
   "metadata": {},
   "source": [
    "### With RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11c90b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'adam', 'max_iter': 5000, 'learning_rate_init': 0.2, 'learning_rate': 'invscaling', 'hidden_layer_sizes': (50,), 'alpha': 0, 'activation': 'logistic'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "\n",
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [ (50,), (70,),(50,30), (40,20), (60,40, 20), (70,50,40)],\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0, .2, .5, .7, 1],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1, 0.2, 0.5],\n",
    "    'max_iter': [5000]\n",
    "}\n",
    "\n",
    "ann = MLPClassifier()\n",
    "grid_search = RandomizedSearchCV(estimator = ann, param_distributions=param_grid, cv=kfolds, n_iter=100,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0fb6ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.66      0.62        65\n",
      "           1       0.80      0.74      0.77       120\n",
      "\n",
      "    accuracy                           0.71       185\n",
      "   macro avg       0.69      0.70      0.69       185\n",
      "weighted avg       0.72      0.71      0.72       185\n",
      "\n",
      "Wall time: 16 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = bestRecallTree.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e4e428",
   "metadata": {},
   "source": [
    "### So, we have accurately recognized 77% of the loan status in our predictions, which are around 77% accurate by using MLPclassifier using random search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0871acee",
   "metadata": {},
   "source": [
    "### With GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ecaf6088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanm\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'tanh', 'alpha': 0.5, 'hidden_layer_sizes': (90,), 'learning_rate': 'adaptive', 'learning_rate_init': 0.01, 'max_iter': 5000, 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [ (30,), (50,), (70,), (90,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [.5, .7, 1],\n",
    "    'learning_rate': ['adaptive', 'invscaling'],\n",
    "    'learning_rate_init': [0.005, 0.01, 0.15],\n",
    "    'max_iter': [5000]\n",
    "}\n",
    "\n",
    "ann = MLPClassifier()\n",
    "grid_search = GridSearchCV(estimator = ann, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7bec52ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.55      0.63        65\n",
      "           1       0.79      0.89      0.84       120\n",
      "\n",
      "    accuracy                           0.77       185\n",
      "   macro avg       0.76      0.72      0.73       185\n",
      "weighted avg       0.77      0.77      0.76       185\n",
      "\n",
      "Wall time: 19 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = bestRecallTree.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb9f145",
   "metadata": {},
   "source": [
    "### So, we have accurately recognized 77% of the loan status in our predictions, which are around 77% accurate by using MLPclassifier using grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3bb8ce",
   "metadata": {},
   "source": [
    "# By fitting Logistic Regression, SVM (with various kernels),Decision trees and MPL classifier. According to my point of view, DecisionTreeClassifier using random search  is the best fit model as campared to other models, So, we have accurately recognized 83.44% of the loan status in our predictions, which are around 83.44% accurate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
